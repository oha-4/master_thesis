\documentclass[a4paper,11pt,oneside,openany,report,dvipdfmx]{jsbook}

\usepackage[left=30mm,right=30mm,top=25mm,bottom=25mm]{geometry}
\usepackage{cscover}
\usepackage{graphicx}
\usepackage[nobreak]{cite}
\usepackage[a4paper,dvipdfmx,pdfdisplaydoctitle=true,%
	bookmarks=true,bookmarksnumbered=true,bookmarkstype=toc,bookmarksopen=true,%
	pdftitle={Pre-Markingアクションを用いた\\Directed Controller Synthesisの\\探索ヒューリスティック改善},%
	pdfauthor={大畑允人}%
]{hyperref}
\usepackage{pxjahyper}

\renewcommand{\headfont}{\bfseries}
\renewcommand{\bibname}{参考文献}
\setcounter{tocdepth}{2}
\pagestyle{plain}

\thesistype{修士論文}
\title{Pre-Markingアクションを用いた\\Directed Controller Synthesisの\\探索ヒューリスティック改善}
\author{大畑 允人}
\studentid{24M30552}
\affiliation{%
	東京科学大学\\
	情報理工学院\\
	情報工学コース
}
\date{2026年1月}

\supervisorname{指導教員}
\supervisor{鄭 顕志}

\usepackage{algorithm}
\usepackage[italicComments=false,indLines=false]{algpseudocodex}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{thmtools}

\declaretheoremstyle[
	notefont=\mdseries, notebraces={（}{）},
	headpunct=．,
]{jstyle}
\declaretheorem[name=定義,style=jstyle]{definition}

\newcommand{\descitem}[1]{\item[#1]\mbox{}\\}

\makeatletter

\renewcommand{\fps@algorithm}{ht}
\renewcommand{\ALG@name}{アルゴリズム}
\renewcommand{\algorithmicrequire}{\textbf{入力：}}
\renewcommand{\algorithmicensure}{\textbf{出力：}}
\algrenewcommand\Return{\State\textbf{return} }
\algnewcommand\Break{\State\textbf{break}}
\algnewcommand\Continue{\State\textbf{continue}}

\makeatother

\begin{document}

\frontmatter

\maketitle

\chapter{概要}

概要

\tableofcontents
\listoffigures
\listoftables

\mainmatter

\chapter{序論}

現代の社会インフラや産業システムにおいて，ソフトウェアが担う役割は拡大の一途をたどっている．
これらのシステムの多くは，離散的な状態と，その状態を遷移させる事象（アクション）の列によって振る舞いが記述される離散事象システム（Discrete Event System, DES）としてモデル化できる．
システムの高機能化・複雑化に伴い，システムが安全性や活性といった要求仕様を確実に満たすことを保証するのはますます困難になっており，設計段階における支援技術の重要性が高まっている．

本章では，離散事象システムとその制御に関する背景を述べ，本研究が取り組む課題と目的，および本論文の構成について概説する．

\section{離散事象システムと離散制御器合成}

離散事象システム（DES）は，離散的な状態空間を持ち，非同期に発生する事象によって状態遷移が引き起こされる動的システムである．
製造ラインの工程管理，ロボットの動作計画，通信プロトコル，組み込みシステムなど，その適用範囲は多岐にわたる．
これらのシステムに対し，危険な状態に到達しないというSafetyや，目的とするタスクを無限回完了できるというNon-Blockingを保証することは，システムの信頼性を確保する上で不可欠である．

従来，これらの要求を満たす制御ロジック（動作仕様）の設計は，熟練した技術者の経験や手動による試行錯誤に依存していた．
しかし，並行して動作する複数のコンポーネントが複雑に相互作用するシステムにおいて，人間の直感だけで全てのコーナーケースを網羅し，デッドロックや禁止状態への到達を防ぐことは極めて困難である．
設計ミスは手戻りのコストを増大させるだけでなく，運用時の重大な事故につながるリスクも孕んでいる．

この課題に対する解決策として，形式手法に基づき，与えられた環境モデルと要求仕様から正しい制御器を自動生成する離散制御器合成（Discrete Controller Synthesis）の研究が進められている~\cite{}．
離散制御器合成は，環境の可能な振る舞いをすべて考慮した上で，制御可能なアクションを適切に許可・禁止することで，システムが常に仕様を満たすことを数学的に保証する．
特に，システム自身が能動的にアクションを選択してタスクを遂行するようなシステムにおいては，各状態で高々1つの制御アクションのみを選択するDirected Controllerの合成が有効である~\cite{}．
このDirected Controllerを合成する技術をDirected Controller Synthesis（DCS）と呼ぶ．

しかし，DCSの実適用における最大の障壁は状態空間爆発問題である．
システムを構成するコンポーネント数が増加すると，合成後の状態空間は指数関数的に増大する．
これに対処するため，状態空間全体を事前に構築せず，初期状態から必要な部分のみを探索するOn-The-Fly探索手法や，探索を効率化するためのヒューリスティック技術が提案されてきた~\cite{}．
特に，Ciolekらが提案したReady Abstraction（RA）は，並列合成の構造を利用して目標までの距離を効率的に推定する強力なヒューリスティックであり，DCSの適用範囲を拡大させてきた．

\section{本論文の目的}

本論文の目的は，DCSにおけるOn-The-Fly探索の効率をさらに向上させるため，従来のReady Abstraction（RA）が抱える構造的な課題を解決する新たな探索指針であるPre-Marking Directionを提案することである．

従来のRAは，探索中に到達したMarked状態への距離を推定し，そこへの再訪問を促すことで効率よく制御器合成することを目指す．
しかし，実際のシステム要件としてMarkingアクションが与えられた場合，これをMarked状態の問題に変換してRAを適用すると，以下の2つの重大な課題が生じ，探索効率が著しく低下することが判明している．

\begin{description}
  \descitem{探索到達済みMarked状態による誘導の不整合}
  MarkingアクションをMarked状態の問題へ変換した場合，Marked状態となるのはMarkingアクションが発火した直後の状態である．RAはこの発火後の状態への距離を最小化しようとするが，Markingアクションを再び発火させるために真に近づくべきは，そのアクションが有効化される発火の1歩手前の状態である．この目標地点のズレにより，RAのヒューリスティック値が適切なガイダンスとならず，探索の迷走を招く．
  \descitem{Readyアクションに基づく距離推定の近視眼性}
  RAは，現在の状態で局所的に発火可能（Ready）なアクションのみをノードとするグラフを構築して距離を推定する．そのため，現在はReadyではないが，将来的にMarkingアクションを発火させるために必須となる遠くにあるアクションが距離計算の考慮から漏れてしまう．結果として，長期的には有望な経路が無視され，手近なアクションばかりが選択され，計算リソースを浪費する．
\end{description}

そこで本研究では，Markingアクションそのものに着目し，それが発火可能になるまでの予備動作（Pre-Markingアクション）を考慮した新たな距離推定手法を提案する．
具体的には，以下の項目に取り組む．

\begin{description}
  \descitem{Markingアクション環境下におけるRAの課題分析}
  上述した探索到達済みMarked状態による誘導の不整合と，Readyアクション限定による遠方アクションの無視という2つの課題を，具体的なモデルを用いて明らかにする．
  \descitem{Pre-Marking Directionの提案}
  Markingアクションの発火に不可欠な同期アクションをPre-Markingアクションとして定義し，これを探索の道標とする新たなヒューリスティック手法Pre-Marking Direction（PMD）を提案する．
  PMDは，各モデル単独での目標までの局所的な距離に加え，他モデルのPre-Markingアクションへの到達待ち時間を考慮した同期制約下の距離を算出し，両者の最大値を用いて探索優先度を決定する．
  これにより，Ready Abstractionでは考慮が及ばなかったモデル間の同期ボトルネックを明示的に評価し，探索の迷走を防ぐと共に，事前計算を活用した高速なヒューリスティック計算を実現する．
  \descitem{評価実験による有効性の検証}
  提案手法を実装し，ベンチマーク問題を用いて従来手法と比較することで，探索状態数や計算時間の削減効果を定量的に評価する．
\end{description}

本研究の成果は，より大規模で複雑なシステムの制御器合成を現実的な時間・メモリリソースで可能にし，高信頼なシステム開発の自動化に貢献するものである．

\section{本論文の構成}

本論文の構成は以下の通りである．

第2章「背景技術」では，本研究の基礎となる離散制御器合成およびDCSの理論的枠組み，On-The-Fly探索アルゴリズム，および既存のヒューリスティック手法であるReady Abstractionについて詳説する．

第3章「Ready Abstractionの課題」では，Markingアクションを目標とする問題設定において，既存のRAが抱える構造的な限界と，それによって生じる探索効率の低下について分析する．

第4章「Pre-Marking Direction」では，本研究の中核となる提案手法について述べる．Markingアクションの発火を直接的な目標としてRAの距離計算に統合するアルゴリズムを示し，それがどのように探索を効率化するかを説明する．また，提案手法の適用による性能向上を実験により評価する．

第5章「関連研究」では，DCSの効率化やヒューリスティック探索に関する先行研究を概観し，本研究の位置づけを明確にする．

第6章「結論」では，本論文の成果を総括し，今後の展望について述べる．

\chapter{背景技術}

本章では，本研究の基盤となる離散制御器合成の概要，Directed Controller Synthesis（DCS）の定式化とOn-The-Fly探索アルゴリズム，およびReady Abstraction（RA）に基づくヒューリスティック関数について述べる．

\section{離散制御器合成}

離散制御器合成（Discrete Controller Synthesis）は，離散事象システム（Discrete Event System, DES）に対して，与えられた安全性などの制約を満たす制御器を自動的に構築する技術である．
DESは有限状態オートマトン（あるいはLTS）の並列合成によってモデル化されるが，並列合成により得られる状態空間はコンポーネント数に対して指数的に増大しうる．
この指数的爆発は離散制御問題の解決を困難にし，現在も研究上の課題となっている．

制御器は，制御可能なアクションを動的に無効化しながら，制御不能なアクションを監視することで，システムの振る舞いを制限する．
一般のスーパバイザ制御では可能な限り多くの制御可能アクションを有効にする（最大許容）ことが求められることが多い．
しかし，DESの制御器が「制御されたアクションを実行するアクティブなコンポーネント」として振る舞う文脈では，任意の時点で高々1つの制御可能アクションのみを選択する制御が適切となる場合がある．
このような制御器はDirected Controllerと呼ばれる．

\subsection{ラベル付き遷移系}

本研究では，環境モデル・要求モデルをラベル付き遷移系（Labeled Transition System, LTS）で表現する．
なお，本研究ではMarkingアクション（アクションベースの目標）を主に用いるが，既存研究ではMarked状態（状態ベースの目標）を用いることもある．
この両者を扱えるよう，LTSにはMarked状態集合とMarkingアクション集合の両方を持たせる（必要に応じて片方を空集合または全体集合として扱う）．

\begin{definition}[LTS]
  LTSは
  $E = (S_E, s_{E,0}, S^I_E, S^M_E, A_E, A^M_E, \Delta_E)$
  で表現される．
  $S_E$は有限の状態集合であり，$s_{E,0} \in S_E$は初期状態を表す．
  $S^I_E \subseteq S_E$は違反状態の集合であり，システムが到達してはならない状態を表す．
  $S^M_E \subseteq S_E$はMarked状態の集合である（RAや既存のDCS定式化で用いる）．
  $A_E = A^C_E \cup A^U_E$はアクション集合であり，$A^C_E$は制御可能アクション，$A^U_E$は制御不能アクションを表す．
  $A^M_E \subseteq A_E$はMarkingアクション集合である（本研究の定式化で用いる）．
  $\Delta_E \subseteq S_E \times A_E \times S_E$は遷移関係を表す．
\end{definition}

\begin{definition}[決定性]
  LTS $E$が決定的であるとは，任意の状態$s\in S_E$とアクション$a\in A_E$について，
  $(s,a,s_1)\in\Delta_E$かつ$(s,a,s_2)\in\Delta_E$ならば$s_1=s_2$が成り立つことをいう．
\end{definition}

本研究では，全てのLTSは決定的であると仮定する．

\begin{definition}[トレース]
  $\Delta_E$に従う状態とアクションの有限または無限の交互列
  $t = s_0, a_0, s_1, a_1, \ldots$
  をトレースと呼ぶ．
  ただし，各$i$について$(s_i, a_i, s_{i+1}) \in \Delta_E$が成り立つ．
  $E$上のトレースの集合を$T_E$で表す．
\end{definition}

\subsection{同期アクションと並列合成}

\begin{definition}[同期アクション]
  複数のLTS $E_1, \ldots, E_n$において，アクション$a$が2つ以上のコンポーネントのアクション集合に含まれるとき，
  $a$を同期アクションと呼ぶ．同期アクション集合$A^S$は
  $$
    A^S = \left\{a \in \bigcup_{i=1}^{n} A_{E_i} \;\middle|\; |\{i \mid a \in A_{E_i}\}| \geq 2\right\}
  $$
  と定義する．
\end{definition}

\begin{definition}[並列合成]
  LTS集合$\mathcal{E} = \{E_1,\ldots,E_n\}$の並列合成$E_\parallel = E_1 \parallel E_2 \parallel \cdots \parallel E_n$を$E_\parallel = (S_{E_\parallel}, s_{E_\parallel ,0}, S^I_{E_\parallel}, S^M_{E_\parallel}, A_{E_\parallel}, A^M_{E_\parallel}, \Delta_{E_\parallel})$
  として定義する．
  状態集合$S_{E_\parallel}=\prod_{i=1}^{n}S_{E_i}$は各コンポーネントの状態の直積であり，初期状態は$s_{E_\parallel,0}=(s_{E_1,0},\ldots,s_{E_n,0})$である．
  違反状態集合$S^I_{E_\parallel}$は，いずれかのコンポーネントが違反状態にある状態の集合$\{(s_1, \ldots, s_n) \in S_{E_\parallel} \mid \exists i . s_i \in S^I_{E_i}\}$として定義される．
  Marked状態集合は$S^M_{E_\parallel}=\prod_{i=1}^{n}S^M_{E_i}$，
  アクション集合$A_{E_\parallel}=\bigcup_{i=1}^{n}A_{E_i}$，
  Markingアクション集合$A^M_{E_\parallel}=\bigcup_{i=1}^{n}A^M_{E_i}$である．
  遷移関係$\Delta_{E_\parallel}$については，同期アクション$a \in A^S$は$a$を持つ全てのLTSで同時に発火し，非同期アクション$a \notin A^S$は$a$を持つLTSのみで発火する．
\end{definition}

並列合成には以下の特徴がある：
\begin{itemize}
  \item 同期アクションによりコンポーネント間の協調動作を実現できる
  \item 状態数$|S_{E_\parallel}|$はLTS数に対して指数的に増大しうる
\end{itemize}

\section{Directed Controller Synthesis}

本節では，Directed Controller Synthesis（DCS）の形式的定義と，On-The-Fly探索によるアルゴリズムについて述べる．

\subsection{制御可能性とDirected Controller}

アクションには制御可能なものと制御不能なものがある．
制御可能アクションは，制御器によって有効化・無効化が可能である．
一方，制御不能なアクションは環境によって発火されるため，発火し得る場合は全ての発火を考慮しなければならない．
各状態の制御可能なアクションのうち高々1つのみを有効にする制御器を合成することで，システム全体の振る舞いを制御する．
このような制御器をDirected Controllerと呼ぶ．

\begin{definition}[Directed Controller]
  並列合成LTS $E_\parallel$に対する制御器が以下の条件を満たすとき，Directed Controllerと呼ぶ：
  \begin{description}
    \item[Controllable]
          すべての制御不能アクションを常に有効化する
    \item[Directed]
          各状態で高々1つの制御可能アクションのみを有効化する
    \item[Eager]
          Marked状態，またはMarkingアクション発火後の状態において，制御可能アクションのみ存在する場合，必ず1つを選択する
  \end{description}
\end{definition}

\subsection{制御問題}

既存のDCSでは，タスク完了などを表すMarked状態への到達可能性に基づいてNon-Blocking性を定義することがある．
本研究のMarkingアクションベース定式化との関係は\ref{subsec:marking_to_marked}節で述べる．
また，本研究では，目標を状態ではなくMarkingアクション$A^M$の発火として与える．

\begin{definition}[Directed Controllerの制御問題]
  LTSの組$\bm{E}=(E_1,\ldots,E_n)$に対し，解は以下を満たすDirected Controllerである：
  \begin{description}
    \item[Safety]
          制御下のシステムが違反状態に到達しない
    \item[Non-Blocking]
          Marked状態への到達，またはMarkingアクションの発火を無限回行える
  \end{description}
\end{definition}

\subsection{MarkingアクションからMarked状態への変換}
\label{subsec:marking_to_marked}

本研究ではMarked状態を用いずMarkingアクション集合$A^M$を用いるが，Marked状態を無限回訪れる問題へ変換する．
この変換は，以下のようにLTSを1つ追加することで実現される：

\begin{itemize}
  \item 各$E_i$について，Marked状態集合を$S^M_{E_i}\gets S_{E_i}$（全状態Marked）とする
  \item LTS $E_M$を追加する：
        \begin{itemize}
          \item 状態集合 $S_{E_M}=\{0,1\}$，初期状態$0$，Marked状態$S^M_{E_M} = \{1\}$
          \item 任意のMarkingアクション$a \in A^M$に対し $0 \xrightarrow{a} 1,\ 1 \xrightarrow{a} 1$
          \item 任意の非Markingアクション$a \notin A^M$に対し $0 \xrightarrow{a} 0,\ 1 \xrightarrow{a} 0$
        \end{itemize}
\end{itemize}

このとき，合成対象を$E_1 \parallel E_2 \parallel \cdots \parallel E_n \parallel E_{M}$とみなすと，
全体がMarkedであるかどうかは$E_M$が状態1にいるかどうかのみに依存する．
$E_M$は直前の遷移がMarkingアクションであるときのみ状態1に滞在できるため，
変換後のMarked状態を無限回訪問することは，変換前のMarkingアクションを無限回発火することと等価になる．

\subsection{On-The-Fly探索アルゴリズム}

従来のアプローチでは，まずオートマトンを完全に合成し，その後で制御問題を解く．
しかし，この方法は指数的な状態空間爆発を引き起こす可能性がある．
Ciolekら~\cite{}は，ヒューリスティックに導かれたOn-The-Fly探索により，状態空間の一部のみを探索してDirected Controllerを発見するアルゴリズムを提案した．

On-The-Fly探索は，状態空間全体を事前に構築することなく，初期状態から到達可能な必要な部分のみを段階的に展開しながら制御器を合成する手法である．
この手法は，主に以下の3つのプロセスから構成される．

\begin{enumerate}
  \item \textbf{展開（Expansion）}:
        現在の探索フロンティアから，ヒューリスティック関数（例えばRA）を用いて最も有望な未探索の遷移を選択し，探索グラフに追加する（\textsc{ExpandNext}）．これにより，Marked状態への到達可能性が高い経路を優先的に探索する．
  \item \textbf{伝播（Propagation）}:
        ある状態が\textit{Goals}（勝利状態）または\textit{Errors}（敗北状態）であることが確定した場合，その情報を探索グラフの逆方向（親状態）へ伝播させる（\textsc{PropagateGoal}, \textsc{PropagateError}）．
        具体的には，ある状態から到達可能なすべての遷移先が\textit{Errors}であればその状態も\textit{Errors}となり，逆に制御可能な遷移によって\textit{Goals}へ到達可能であればその状態も\textit{Goals}となる．
  \item \textbf{閉路検出と判定（Loop Detection）}:
        探索中に新たな閉路が形成された場合，その閉路がNon-Blockingを満たすか否かを判定する．
        Markingアクションを含む閉路などの「勝利閉路」が見つかれば，その閉路上の状態は\textit{Goals}となり，逆にMarkingアクションを含まず脱出不可能な閉路は\textit{Errors}となる．
\end{enumerate}

探索は初期状態が\textit{Goals}または\textit{Errors}に分類されるまで継続される．初期状態が\textit{Goals}に分類された場合，探索グラフから有効な部分グラフを抽出することで制御器が得られる．逆に\textit{Errors}に分類された場合は，制御器の合成は不可能であると結論付けられる．

\begin{algorithm}
  \caption{On-The-Fly探索によるDirected Controller Synthesis}
  \label{alg:dcs}
  \begin{algorithmic}[1]
    \Require{全てのLTSの組 $\bm{E} = (E_1, \ldots, E_n)$，ヒューリスティック関数 $H$}
    \Ensure{Directed Controller $C$ または 合成不可能}

    \Function{DirectedControllerSynthesis}{$\bm{E}, H$}

    \State $\textit{ES} \gets$初期状態$\bm{s}_0$のみを持つ部分探索グラフ
    \State $\textit{Goals} \gets \emptyset, \textit{Errors} \gets \emptyset, \textit{None} \gets \{\bm{s}_0\}$

    \Statex

    \LComment{探索ループ：初期状態が未分類状態でなくなるまで}
    \While{$\bm{s}_0 \notin (\textit{Goals} \cup \textit{Errors})$}
      \State $(\bm{s}, a, \bm{s}') \gets \textsc{ExpandNext}(\textit{ES}, H)$ \Comment{ヒューリスティックによる次遷移の選択}
      \State $\textit{ES} \gets \textit{ES} \cup \{(\bm{s}, a, \bm{s}')\}$ \Comment{探索グラフの更新}

      \Statex
      \If{$\bm{s}' \in \textit{Errors}$}
        \State $\textit{Errors} \gets \textit{Errors} \cup \{\bm{s}'\}$
        \State $\textsc{PropagateError}(\bm{s}')$ \Comment{敗北状態の伝播}
      \ElsIf{$\bm{s}' \in \textit{Goals}$}
        \State $\textit{Goals} \gets \textit{Goals} \cup \{\bm{s}'\}$
        \State $\textsc{PropagateGoal}(\bm{s}')$ \Comment{勝利状態の伝播}
      \ElsIf{$\bm{s}'$ が新しいループを形成する}
        \State $\textit{Loop} \gets \textsc{GetLoop}(\bm{s}, \bm{s}')$
        \If{$\textit{Loop}$ が勝利条件を満たす}
          \State $\textit{newGoals} \gets \textsc{FindNewGoals}(\textit{Loop})$
          \State $\textit{Goals} \gets \textit{Goals} \cup \textit{newGoals}$
          \State $\textsc{PropagateGoal}(\textit{newGoals})$
        \Else
          \State $\textit{newErrors} \gets \textsc{FindNewErrors}(\textit{Loop})$
          \State $\textit{Errors} \gets \textit{Errors} \cup \textit{newErrors}$
          \State $\textsc{PropagateError}(\textit{newErrors})$
        \EndIf
      \EndIf
    \EndWhile

    \Statex

    \If{$s_0 \in \textit{Goals}$}
      \State $C \gets \textsc{ExtractController}(\textit{ES}, \textit{Goals})$
      \Return $C$
    \Else
      \Return 合成不可能
    \EndIf

    \EndFunction

  \end{algorithmic}
\end{algorithm}

\section{Ready Abstraction}

Ready Abstraction（RA）は，並列合成構造を活用して，Marked状態への到達に必要なステップ数を多項式時間で推定するヒューリスティック手法である．
各コンポーネントの局所情報のみを用いて距離を推定することで状態空間爆発を回避しつつ，On-The-Fly探索における次遷移選択（\textsc{ExpandNext}）を導く．

RAの基本的な着想は，合成状態 $\bm{s} = (s_{E_1}, s_{E_2}, \ldots, s_{E_n})$ の周辺で局所的に発火可能なアクション（Readyアクション）を集め，それらの間に，あるアクションの実行が別のアクションの実行可能性を高めるという関係を仮想的に張ったグラフ上で最短路を解くことで，目標（Marked状態）までの距離を見積もる点にある．
同期制約のため，局所的に発火可能（Ready）であっても合成状態全体では直ちに発火できない場合があるが，RAはこの差を局所到達可能性とグラフ探索に基づいて保守的に見積もる．

\subsection{候補アクションの評価}

アルゴリズム\ref{alg:ready_abstraction}は，現在の合成状態 $\bm{s}$ において候補アクション $\hat{a}$ を選ぶことがどれだけ目標に近いかを評価するためのヒューリスティック関数である．
評価は各コンポーネント $E_i$ ごとに $(\mathrm{rank}, d)$ の形で返され，rank は距離推定に用いる目標集合の優先度を表す．

本章で示すRAの記述では，探索中にすでに到達したMarked状態集合 $\bm{S}^M$ を基準にした距離（Rank 0）を優先する．
これは，探索が進むほど既知の到達済み領域へ近づく選択を上位に扱うことで，任意のMarked状態を一様に目指す場合に比べて探索が過度に広がることを避け，探索空間を抑制できるという見込みに基づく．
Rank 0での推定が不可能な場合に限り，任意のMarked状態への距離（Rank 1）を用いる．
どちらも不可能と推定された場合は $(2,\infty)$ を返す．

\begin{algorithm}[H]
  \caption{Ready Abstractionによるヒューリスティック関数}
  \label{alg:ready_abstraction}
  \begin{algorithmic}[1]
    \Require{%
      全てのLTSの組 $\bm{E} = (E_1, E_2, \ldots, E_n)$， \\
      探索到達済みのMarked状態の集合 $\bm{S}^M \subseteq S^M_{E_1} \times S^M_{E_2} \times \cdots \times S^M_{E_n}$， \\
      各LTSにおける状態の組 $\bm{s} = (s_{E_1}, s_{E_2}, \ldots, s_{E_n})$， \\
      候補のアクション $\hat{a}$
    }
    \Ensure{%
      各LTSにおける距離の降順の組 $\bm{d} = (\bm{d}_1, \bm{d}_2, \ldots, \bm{d}_n)$ \\
      ただし，$\bm{d}_i \in \{(0, d), (1, d), (2, \infty) \mid d \in \mathbb{N}\}$ \\
      各LTSにおける距離の意味は以下の通り： \\
      $(0, d)$：次にアクション$\hat{a}$を発火して$d$ステップで探索到達済みのMarked状態に到達可能 \\
      $(1, d)$：次にアクション$\hat{a}$を発火して$d$ステップでMarked状態に到達可能 \\
      $(2, \infty)$：Marked状態に到達不可能
    }

    \Function{ReadyAbstractionHeuristic}{$\bm{E}, \bm{S}^M, \bm{s}, \hat{a}$}

    \For{$i = 1, 2, \ldots, n$}
      \State $\textit{minSteps} \gets \infty$

      \Statex

      \LComment{Rank 0: 探索到達済みのMarked状態への到達可能性確認}
      \ForAll{$(s^*_{E_1}, s^*_{E_2}, \ldots, s^*_{E_n}) \in \bm{S}^M$}
        \State $\textit{steps} \gets \textsc{EstimateDist}(\bm{s}, \hat{a}, s^*_{E_i})$
        \If{$\textit{steps} < \textit{minSteps}$}
          $\textit{minSteps} \gets \textit{steps}$
        \EndIf
      \EndFor
      \If{$\textit{minSteps} \ne \infty$} \Comment{いずれかの探索到達済みのMarked状態に到達可能な場合}
        \State $\bm{d}_{E_i} \gets (0, minSteps)$
        \Continue
      \EndIf

      \Statex

      \LComment{Rank 1: 任意のMarked状態への到達可能性確認}
      \ForAll{$s^*_{E_i} \in S^M_{E_i}$}
        \State $\textit{steps} \gets \textsc{EstimateDist}(\bm{s}, \hat{a}, s^*_{E_i})$
        \If{$\textit{steps} < \textit{minSteps}$}
          $\textit{minSteps} \gets \textit{steps}$
        \EndIf
      \EndFor

      \If{$\textit{minSteps} \ne \infty$} \Comment{いずれかのMarked状態に到達可能な場合}
        \State $\bm{d}_{E_i} \gets (1, minSteps)$
      \Else
        \State $\bm{d}_{E_i} \gets (2, \infty)$
      \EndIf
    \EndFor

    \Statex

    \Return $\textsc{SortDescending}((\bm{d}_{E_1}, \bm{d}_{E_2}, \ldots, \bm{d}_{E_n}))$
    \Comment{距離を辞書式降順でソートして返す}

    \EndFunction
  \end{algorithmic}
\end{algorithm}

アルゴリズム\ref{alg:ready_abstraction}の戻り値は，各コンポーネントに関する距離推定の組であるため，実際の探索ではこの組を辞書式に比較して候補アクションを順位付けするのが自然である．
例えば，まずrankの小さいものを優先し，rankが同じ場合には距離$d$が小さいものを優先する，といった運用ができる．
このように組の比較として扱うことで，並列合成の構造を保ったまま候補を評価できる．

\subsection{距離推定の中核}

アルゴリズム\ref{alg:estimate_dist}は，アルゴリズム\ref{alg:ready_abstraction}が内部で呼び出す距離推定関数である．
入力は現在の合成状態 $\bm{s}$，次に実行する候補アクション $\hat{a}$，距離の対象となるコンポーネント状態 $s^*_{E_i}$ であり，出力は推定距離 $d$ である．

推定は大きく二段階で行われる．
まず，$\hat{a}$ がコンポーネント $E_i$ に存在するなら，$\hat{a}$ を局所的に一回発火した後の状態 $s'_{E_i}$ を確認し，そこからの局所最短距離 $\textsc{CalculateDist}(s'_{E_i}, s^*_{E_i})$ を用いて直接効果を見積もる．
これで判断できない場合には，現在の合成状態から（同期を無視すれば）状態を変化させ得るアクション集合 $A^R$ を列挙し，切り替えコスト（Gap）とその先の推定距離の和の最小値として距離を与える．
この再帰的最小化は，実装上はRAグラフ上の最短路として扱うことで効率化できる．

\begin{algorithm}
  \caption{Ready Abstractionにおける距離推定関数}
  \label{alg:estimate_dist}
  \begin{algorithmic}[1]
    \Require{%
      現在の状態 $\bm{s} = (s_{E_1}, \ldots, s_{E_n})$， \\
      候補のアクション $\hat{a}$， \\
      距離を推定する対象の状態 $s^*_{E_i}$
    }
    \Ensure{推定距離 $d \in \mathbb{N} \cup \{\infty\}$}

    \Function{EstimateDist}{$\bm{s}, \hat{a}, s^*_{E_i}$}

    \If{$\hat{a} \in A_{E_i}$} \Comment{$\hat{a}$ がLTS $E_i$ に存在している場合}
      \State $s'_{E_i} \gets s' \text{ where } (s_{E_i}, \hat{a}, s') \in \Delta_{E_i}$ \Comment{$\hat{a}$ を発火した後の状態}
      \If{$s'_{E_i} = s^*_{E_i}$} \Comment{対象の状態に到達する場合}
        \Return $1$
      \ElsIf{$s'_{E_i} \in S^I_{E_i}$} \Comment{違反状態に到達する場合}
        \Return $\infty$
        \Comment{距離を無限と推定し，Marked状態に到達不可能であることを表す}
      \ElsIf{$s'_{E_i} \ne s_{E_i}$} \Comment{他の状態に遷移する場合}
        \Return $\textsc{CalculateDist}(s'_{E_i}, s^*_{E_i}) + 1$
      \EndIf
    \ElsIf{$s_{E_i} = s^*_{E_i}$} \Comment{$\hat{a}$で遷移が発生せず，すでに対象の状態の場合}
      \Return $1$
    \EndIf

    \Statex

    \LComment{各LTS上で，現在の状態から発火可能なアクションのうち，自己ループでないものの集合}
    \For{$j = 1, 2, \ldots, n$}
      \State $A^R_{E_j} \gets \{a \mid \exists s', (s_{E_j}, a, s') \in \Delta_{E_j}, s' \neq s_{E_j}\}$
    \EndFor
    \State $A^R \gets \bigcup_{j=1}^{n} A^R_{E_j}$ \Comment{全LTSで，同期を考慮しないと発火可能（Ready）なアクションの集合}

    \Statex

    \LComment{Gapを加味した最短経路探索}
    \State $d \gets \min_{a \in A^R}\{\textsc{CalculateGap}(\hat{a}, a) + \textsc{EstimateDist}(\bm{s}, a, s^*_{E_i})\}$
    \LComment{この再帰的な最小値問題は，実装上はダイクストラ法により効率的に解かれる}

    \If{$d = \infty$} \Comment{Readyアクションを用いて距離を推定できない場合}
      \State $d \gets \textsc{CalculateDist}(s_{E_i}, s^*_{E_i}) + 1$
    \EndIf

    \Return $d$

    \EndFunction
  \end{algorithmic}
\end{algorithm}

ここで用いられる \textsc{CalculateGap}$(\hat{a}, a)$ は，アクション間の切り替えのしやすさを表すコストであり，既存研究における定義 $G_E^e(\hat{a}, a)$ に従い，あるコンポーネント内でアクション $\hat{a}$ から $a$ へ至る最短パスの長さ（から1を引いた値）として計算される．

\subsection{Markingアクションまでの距離推定}

RAは，Marked状態への到達距離を推定する枠組みとして提案されている．
一方，本研究では目標をMarkingアクションの発火として定式化するため，RAを本研究の目的関数と同じ観点で比較するには，Marked状態ではなくMarkingアクションまでの距離をどのように見積もるかを明示しておく必要がある．
そこで本節では，Markingアクション集合 $A^M$ に対する距離推定手続きとして\textsc{EstimateDistToMarking}を示す．

\textsc{EstimateDistToMarking}は，候補アクション$\hat{a}$がMarkingアクションであれば距離1を返し，そうでなければ，合成状態$\bm{s}$から（同期を無視すれば）状態を変化させ得るアクション集合$A^R$を介して，Gapとその先での推定距離の和の最小値を再帰的に求める．
構造はアルゴリズム\ref{alg:estimate_dist}（\textsc{EstimateDist}）と同型であり，終端条件がMarked状態ではなくMarkingアクションになっている点が相違である．

なお，\textsc{EstimateDistToMarking}は，探索到達済みMarked状態集合$\bm{S}^M$を基準とするRank 0の推定が利用できない状況，特に$\bm{S}^M=\emptyset$の場合に用いることを想定している．


\begin{algorithm}
  \caption{Ready AbstractionにおけるMarkingアクションまでの距離推定}
  \begin{algorithmic}[1]
    \Require{%
      現在の状態 $\bm{s} = (s_{E_1}, \ldots, s_{E_n})$， \\
      候補のアクション $\hat{a}$
    }
    \Ensure{推定距離 $d \in \mathbb{N} \cup \{\infty\}$}

    \Function{EstimateDistToMarking}{$\bm{s}, \hat{a}$}

    \If{$\hat{a} \in A^M$}
      \Return $1$
    \EndIf

    \LComment{各LTS上で，現在の状態から発火可能なアクションのうち，自己ループでないものの集合}
    \For{$j = 1, 2, \ldots, n$}
      \State $A^R_{E_j} \gets \{a \mid \exists s', (s_{E_j}, a, s') \in \Delta_{E_j}, s' \neq s_{E_j}\}$
    \EndFor
    \State $A^R \gets \bigcup_{j=1}^{n} A^R_{E_j}$ \Comment{全LTSで，同期を考慮しないと発火可能（Ready）なアクションの集合}

    \Return $\min_{a \in A^R \cup A^M}\{\textsc{CalculateGap}(\hat{a}, a) + \textsc{EstimateDistToMarking}(\bm{s}, a)\}$

    \EndFunction
  \end{algorithmic}
\end{algorithm}

以上により，RAは局所情報から，次にどのアクションを展開するのが有望かを推定する枠組みを提供する．
On-The-Fly探索側では，これらの推定値を用いて展開順序を制御することで，合成状態空間の全探索を避けつつ，目的を満たす制御器の発見を狙う．

\chapter{Ready Abstractionの課題}

\chapter{Pre-Marking Direction}

\chapter{関連研究}

\chapter{結論}

\section{本論文のまとめ}

\section{将来研究}

\backmatter

\chapter{謝辞}

\bibliographystyle{jplain}
\bibliography{references}

\end{document}
