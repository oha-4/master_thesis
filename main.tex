\documentclass[a4paper,11pt,oneside,openany,report,dvipdfmx]{jsbook}

\usepackage[left=30mm,right=30mm,top=25mm,bottom=25mm]{geometry}
\usepackage{cscover}
\usepackage{graphicx}
\usepackage[nobreak]{cite}
\usepackage[a4paper,dvipdfmx,pdfdisplaydoctitle=true,%
	bookmarks=true,bookmarksnumbered=true,bookmarkstype=toc,bookmarksopen=true%
]{hyperref}
\usepackage{pxjahyper}

\hypersetup{
  pdftitle={Pre-Markingアクションを用いたDirected Controller Synthesisの探索ヒューリスティック改善},
  pdfauthor={大畑允人}
}

\renewcommand{\headfont}{\bfseries}
\renewcommand{\bibname}{参考文献}
\setcounter{tocdepth}{2}
\pagestyle{plain}

\thesistype{修士論文}
\title{Pre-Markingアクションを用いた\\Directed Controller Synthesisの\\探索ヒューリスティック改善}
\author{大畑 允人}
\studentid{24M30552}
\affiliation{%
	東京科学大学\\
	情報理工学院\\
	情報工学コース
}
\date{2026年1月}

\supervisorname{指導教員}
\supervisor{鄭 顕志}

\usepackage{algorithm}
\usepackage[italicComments=false,indLines=false]{algpseudocodex}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{comment}
\usepackage{mathtools}
\usepackage{thmtools}
\usepackage{tikz}
\usepackage{todonotes}
\usepackage{prettyref}

\declaretheoremstyle[
	notefont=\mdseries, notebraces={（}{）},
	headpunct=．,
]{jstyle}
\declaretheorem[name=定義,style=jstyle]{definition}

\newcommand{\descitem}[1]{\item[#1]\mbox{}\\}

\usetikzlibrary{arrows.meta,automata,positioning}

\newrefformat{chap}{第\ref*{#1}章}
\newrefformat{sec}{第\ref*{#1}節}
\newrefformat{subsec}{第\ref*{#1}項}
\newrefformat{subsubsec}{第\ref*{#1}目}
\newrefformat{fig}{図\ref*{#1}}
\newrefformat{alg}{アルゴリズム\ref*{#1}}
\newcommand\aref[1]{\hyperref[#1]{\prettyref{#1}}}

\setuptodonotes{inline,backgroundcolor=red!15,bordercolor=red}
\newcommand{\memo}[1]{\todo[inline,backgroundcolor=green!15,bordercolor=green]{#1}}

\makeatletter

\renewcommand{\fps@algorithm}{H}
\renewcommand{\ALG@name}{アルゴリズム}
\renewcommand{\algorithmicrequire}{\textbf{入力：}}
\renewcommand{\algorithmicensure}{\textbf{出力：}}
\algrenewcommand\Return{\State\textbf{return} }
\algnewcommand\Break{\State\textbf{break}}
\algnewcommand\Continue{\State\textbf{continue}}

\makeatother

\begin{document}

\frontmatter

\maketitle

\chapter{概要}

\tableofcontents
\listoffigures
\listoftables

\mainmatter

\chapter{序論}

現代の社会インフラや産業システムにおいて，ソフトウェアが担う役割は拡大の一途をたどっている．
これらのシステムの多くは，離散的な状態と，その状態を遷移させる事象（アクション）の列によって振る舞いが記述される離散事象システム（Discrete Event System, DES）としてモデル化できる．
システムの高機能化・複雑化に伴い，システムが安全性や活性といった要求仕様を確実に満たすことを保証するのはますます困難になっており，設計段階における支援技術の重要性が高まっている．

本章では，離散事象システムとその制御に関する背景を述べ，本研究が取り組む課題と目的，および本論文の構成について概説する．

\section{離散事象システムと離散制御器合成}

離散事象システム（Discrete Event System, DES）は，離散的な状態空間を持ち，非同期に発生する事象によって状態遷移が引き起こされる動的システムである~\cite{DiscreteEventSystem}．
製造ラインの工程管理，ロボットの動作計画，通信プロトコル，組み込みシステムなど，その適用範囲は多岐にわたる．
こうしたシステムにおいて，危険な状態に到達しないというSafetyや，目的とするタスクを無限回完了できるというNon-Blockingを保証することは，システムの信頼性を確保する上で不可欠である．

従来，これらの要求を満たす制御ロジック（動作仕様）の設計は，熟練した設計者の経験則や，試行錯誤的な検証に依存していた．
しかし，並行して動作する複数のコンポーネントが複雑に相互作用するシステムにおいて，人間の直感だけで全ての境界条件や例外ケースを網羅し，デッドロックや禁止状態への到達を防ぐことは極めて困難である．
設計ミスは再設計のコストを増大させるだけでなく，運用時の重大な事故につながるリスクも孕んでいる．

この課題に対する解決策として，形式手法に基づき，与えられた環境モデルと要求仕様から正しい制御器を自動生成する離散制御器合成（Discrete Controller Synthesis）の研究が進められている~\cite{DiscreteControllerSynthesis}．
離散制御器合成は，環境の可能な振る舞いをすべて考慮した上で，制御可能なアクションを適切に許可・禁止することで，システムが常に仕様を満たすことを数学的に保証する．
特に，システム自身が能動的にアクションを選択してタスクを遂行する場合には，各状態で高々1つの制御アクションのみを選択するDirected Controllerの合成が有効である~\cite{DirectedController}．
このDirected Controllerを合成する技術をDirected Controller Synthesis（DCS）と呼ぶ~\cite{DirectedControllerSynthesis}．

しかし，DCSの実適用における最大の障壁は状態空間爆発問題である．
システムを構成するコンポーネント数が増加すると，合成後の状態空間は指数関数的に増大する．
これに対処するため，状態空間全体を事前に構築せず，初期状態から必要な部分のみを探索するOn-The-Fly探索手法や，探索を効率化するためのヒューリスティック技術が提案されてきた~\cite{Heuristic}．
特に，Ciolekらが提案したReady Abstraction（RA）~\cite{ReadyAbstraction}は，並列合成の構造を利用して目標までの距離を効率的に推定する有効なヒューリスティックであり，DCSの適用範囲を拡大させてきた．

\section{本論文の目的}

本論文の目的は，DCSにおけるOn-The-Fly探索の効率をさらに向上させるため，従来のReady Abstraction（RA）が抱える構造的な課題を解決する新たな探索指針であるLandmark Ready Abstractionを提案することである．

従来のRAは，各コンポーネントにおいて局所的に発火可能なアクション（Readyアクション）の情報に基づいて目標までの距離を推定する．
しかし，この推定手法には以下の構造的な課題が存在する．

\begin{description}
  \descitem{同期構造を無視した到達可能性評価の限界}
  RAは，Readyアクションのみをノードとするグラフ上で距離を推定する．
  しかし，目標達成に特定のコンポーネント間の同期が不可欠な場合，現在はReadyアクションではないが将来的に同期を解放するために必要な予備アクションが評価から漏れる．
  その結果，同期ボトルネックを考慮しない局所的な経路選択が行われ，探索空間が不必要に拡大する．
  \descitem{網羅的な距離推定による計算資源の浪費}
  RAはヒューリスティック値を算出する際，現在の状態遷移に関与しないコンポーネントを含めた全てに対し，一律に距離推定を実行する．
  この網羅的な評価プロセスは，システム規模（コンポーネント数）に比例して冗長な計算を繰り返すことを意味し，大規模なシステムにおいて1ステップごとの計算コストを不必要に増大させる要因となる．
\end{description}

そこで本研究では，従来のRAの枠組みに，目標アクションの発火に不可欠なアクション（Pre-Markingアクション）を中間目標（ランドマーク）として組み込む新たな距離推定手法を提案する．
具体的には，以下の項目に取り組む．

\begin{description}
  \descitem{Ready Abstractionの探索特性と課題の分析}
  Readyアクションを用いた距離推定が，同期を要するシステムにおいて局所的な挙動を示す原因と，それによって探索空間が不必要に拡大するメカニズムについて，具体的なモデルを用いて分析する．
  \descitem{Landmark Ready Abstractionの提案}
  目標アクションの発火に不可欠なアクションをPre-Markingアクション（PMA）として定義し，これを探索の指針（ランドマーク）とする新たなヒューリスティック手法Landmark Ready Abstraction（LRA）を提案する．
  LRAは，RAが持つ遷移コストの精密な見積もり能力と，PMAによる大域的な方向付けを統合することで，同期ボトルネックを考慮した適切な経路選択を可能にする．
  さらに，距離推定を行う目標を，その時点で解決すべきPMAのみに厳選することで，1ステップあたりの計算コストの効率化も図る．
  これにより，局所的な評価に基づく冗長な経路探索を抑制し，探索状態数および計算時間を大幅に削減することで，システム全体の合成効率を向上させる．
  \descitem{評価実験による有効性の検証}
  提案手法を実装し，ベンチマーク問題を用いて従来手法と比較することで，探索状態数や計算時間の削減効果を定量的に評価する．
\end{description}

本研究の成果は，より大規模で複雑なシステムの制御器合成を現実的な時間・メモリリソースで可能にし，高信頼なシステム開発の自動化に貢献するものである．

\section{本論文の構成}

本論文の構成は以下の通りである．

\aref{chap:background}「背景技術」では，本研究の基礎となる離散制御器合成およびDCSの理論的枠組み，On-The-Fly探索アルゴリズム，および既存のヒューリスティック手法であるReady Abstractionについて詳説する．

\aref{chap:ready_abstraction_limitations}「Ready Abstractionの課題」では，既存のRAが抱える構造的な限界について述べる．特に，Markingアクションの発火にコンポーネント間の同期が必要な状況において，現状態のReadyアクションのみに基づくRAの推定が，大域的な同期制約を考慮できずに局所解への停留や探索空間の拡大を招くメカニズムについて分析する．

\aref{chap:landmark_ready_abstraction}「Landmark Ready Abstraction」では，本研究の中核となる提案手法について述べる．目標アクションの発火に不可欠なアクション（Pre-Markingアクション）の定義と抽出アルゴリズム，およびそれを用いたヒューリスティック関数について詳述する．また，具体的なモデルを用いた動作例を通じ，提案手法がいかにして同期ボトルネックを考慮した適切な経路選択を行うか，その動作原理を明らかにする．

\aref{chap:evaluation}「評価」では，提案手法の有効性を検証するための評価実験について述べる．ベンチマーク問題を用いた実験により，従来手法との探索状態数および計算時間の比較を行う．さらに，実験結果に基づき，提案手法が有効に機能する条件や計算コストに関する考察を行い，実用的な適用指針について議論する．

\aref{chap:related_works}「関連研究」では，DCSの効率化やヒューリスティック探索に関する先行研究を概観し，本研究の位置づけを明確にする．

\aref{chap:conclusion}「結論」では，本論文の成果を総括し，今後の展望について述べる．

\chapter{背景技術}
\label{chap:background}

本章では，本研究の基盤となる離散制御器合成の概要，Directed Controller Synthesis（DCS）の定式化とOn-The-Fly探索アルゴリズム，およびReady Abstraction（RA）に基づくヒューリスティック関数について述べる．

\section{離散制御器合成}

離散制御器合成（Discrete Controller Synthesis）は，離散事象システム（Discrete Event System, DES）に対して，与えられた安全性などの制約を満たす制御器を自動的に構築する技術である~\cite{DiscreteControllerSynthesis}．
DESは有限状態オートマトン（あるいはLTS）の並列合成によってモデル化されるが，並列合成により得られる状態空間はコンポーネント数に対して指数的に増大しうる．
この指数的爆発は離散制御問題の解決を困難にし，現在も研究上の課題となっている．

制御器は，制御可能なアクションを動的に無効化しながら，制御不能なアクションを監視することで，システムの振る舞いを制限する．
一般のスーパバイザ制御では可能な限り多くの制御可能アクションを有効にする（最大許容）ことが求められることが多い．
しかし，制御器自身がアクションを能動的に実行するアクティブなコンポーネントとして振る舞う場合には，任意の時点で高々1つの制御可能アクションのみを選択する制御が適切となる．
このような制御器はDirected Controllerと呼ばれる~\cite{DirectedController}．

\subsection{ラベル付き遷移系}

本論文における離散制御器合成の定式化として，システムを構成する各コンポーネントをラベル付き遷移系（Labeled Transition System, LTS）としてモデル化する．
なお，本研究ではMarkingアクション（アクションベースの目標）を主に用いるが，既存研究ではMarked状態（状態ベースの目標）を用いることもある．
この両者を扱えるよう，LTSにはMarked状態集合とMarkingアクション集合の両方を持たせる（必要に応じて片方を空集合または全体集合として扱う）．

\begin{definition}[LTS]
  LTSは
  $E = (S_E, s_{E,0}, S^I_E, S^M_E, A_E, A^M_E, \Delta_E)$
  で表現される．
  $S_E$は有限の状態集合であり，$s_{E,0} \in S_E$は初期状態を表す．
  $S^I_E \subseteq S_E$は違反状態の集合であり，システムが到達してはならない状態を表す．
  $S^M_E \subseteq S_E$はMarked状態の集合である（RAや既存のDCS定式化で用いる）．
  $A_E = A^C_E \cup A^U_E$はアクション集合であり，$A^C_E$は制御可能アクション，$A^U_E$は制御不能アクションを表す．
  $A^M_E \subseteq A_E$はMarkingアクション集合である（本研究の定式化で用いる）．
  $\Delta_E \subseteq S_E \times A_E \times S_E$は遷移関係を表す．
\end{definition}

\begin{definition}[決定性]
  LTS $E$が決定的であるとは，任意の状態$s\in S_E$とアクション$a\in A_E$について，
  $(s,a,s_1)\in\Delta_E$かつ$(s,a,s_2)\in\Delta_E$ならば$s_1=s_2$が成り立つことをいう．
\end{definition}

本研究では，全てのLTSは決定的であると仮定する．

\begin{definition}[トレース]
  $\Delta_E$に従う状態とアクションの有限または無限の交互列
  $t = s_0, a_0, s_1, a_1, \ldots$
  をトレースと呼ぶ．
  ただし，各$i$について$(s_i, a_i, s_{i+1}) \in \Delta_E$が成り立つ．
  $E$上のトレースの集合を$T_E$で表す．
\end{definition}

\begin{definition}[有効トレース集合]
  LTS $E$ において，状態 $s \in S_E$ を始点とし，アクション集合 $A \subseteq A_E$ のいずれかの発火をもって終了する有限トレースのうち，途中で違反状態 $S^I_E$ を経由しないものの集合を $T_{s \to A}$ と定義する．
  $$
    T_{s \to A} = \left\{
    (s_0, a_0, \dots, s_n) \in T_E \;\middle|\;
    s_0 = s \land a_{n-1} \in A \land \bigwedge_{i = 0}^n s_i \notin S^I_E
    \right\}
  $$
  この集合が空でない（$T_{s \to A} \neq \emptyset$）とき，状態 $s$ から $A$ へ安全に到達可能である．
\end{definition}

\subsection{同期アクションと並列合成}

\begin{definition}[同期アクション]
  複数のLTS $E_1, \ldots, E_n$において，アクション$a$が2つ以上のコンポーネントのアクション集合に含まれるとき，
  $a$を同期アクションと呼ぶ．同期アクション集合$A^S$は
  $$
    A^S = \left\{a \in \bigcup_{i=1}^{n} A_{E_i} \;\middle|\; |\{i \mid a \in A_{E_i}\}| \geq 2\right\}
  $$
  と定義する．
\end{definition}

\begin{definition}[並列合成]
  LTS集合$\mathcal{E} = \{E_1,\ldots,E_n\}$の並列合成$E_\parallel = E_1 \parallel E_2 \parallel \cdots \parallel E_n$を$E_\parallel = (S_{E_\parallel}, s_{E_\parallel ,0}, S^I_{E_\parallel}, S^M_{E_\parallel}, A_{E_\parallel}, A^M_{E_\parallel}, \Delta_{E_\parallel})$
  として定義する．
  状態集合$S_{E_\parallel}=\prod_{i=1}^{n}S_{E_i}$は各コンポーネントの状態の直積であり，初期状態は$s_{E_\parallel,0}=(s_{E_1,0},\ldots,s_{E_n,0})$である．
  違反状態集合$S^I_{E_\parallel}$は，いずれかのコンポーネントが違反状態にある状態の集合$\{(s_1, \ldots, s_n) \in S_{E_\parallel} \mid \exists i . s_i \in S^I_{E_i}\}$として定義される．
  Marked状態集合は$S^M_{E_\parallel}=\prod_{i=1}^{n}S^M_{E_i}$，
  アクション集合$A_{E_\parallel}=\bigcup_{i=1}^{n}A_{E_i}$，
  Markingアクション集合$A^M_{E_\parallel}=\bigcup_{i=1}^{n}A^M_{E_i}$である．
  遷移関係$\Delta_{E_\parallel}$については，同期アクション$a \in A^S$は$a$を持つ全てのLTSで同時に発火し，非同期アクション$a \notin A^S$は$a$を持つLTSのみで発火する．
\end{definition}

並列合成には以下の特徴がある：
\begin{itemize}
  \item 同期アクションによりコンポーネント間の協調動作を実現できる
  \item 状態数$|S_{E_\parallel}|$はLTS数に対して指数的に増大しうる
\end{itemize}

\section{Directed Controller Synthesis}

本節では，Directed Controller Synthesis（DCS）の形式的定義と，On-The-Fly探索によるアルゴリズムについて述べる．

\subsection{制御可能性とDirected Controller}

アクションには制御可能なものと制御不能なものがある．
制御可能アクションは，制御器によって有効化・無効化が可能である．
一方，制御不能なアクションは環境によって発火されるため，発火し得る場合は全ての発火を考慮しなければならない．
各状態の制御可能なアクションのうち高々1つのみを有効にする制御器を合成することで，システム全体の振る舞いを制御する．
このような制御器をDirected Controllerと呼ぶ．

\begin{definition}[Directed Controller]
  並列合成LTS $E_\parallel$に対する制御器が以下の条件を満たすとき，Directed Controllerと呼ぶ：
  \begin{description}
    \item[Controllable]
          すべての制御不能アクションを常に有効化する
    \item[Directed]
          各状態で高々1つの制御可能アクションのみを有効化する
    \item[Eager]
          Marked状態，またはMarkingアクション発火後の状態において，制御可能アクションのみ存在する場合，必ず1つを選択する
  \end{description}
\end{definition}

\subsection{制御問題}

既存のDCSでは，タスク完了などを表すMarked状態への到達可能性に基づいてNon-Blocking性を定義することがある．
本研究のMarkingアクションベース定式化との関係は\aref{subsec:marking_to_marked}で述べる．
また，本研究では，目標を状態ではなくMarkingアクション$A^M$の発火として与える．

\begin{definition}[Directed Controllerの制御問題]
  LTSの組$\bm{E}=(E_1,\ldots,E_n)$に対し，解は以下を満たすDirected Controllerである：
  \begin{description}
    \item[Safety]
          制御下のシステムが違反状態に到達しない
    \item[Non-Blocking]
          Marked状態への到達，またはMarkingアクションの発火を無限回行える
  \end{description}
\end{definition}

\subsection{MarkingアクションからMarked状態への変換}
\label{subsec:marking_to_marked}

本研究では，目標をMarkingアクション集合 $A^M$ の発火として定義する．
一方，RAはMarked状態への到達可能性に基づくヒューリスティックである．
そのため，本研究の定式化をRAの枠組みに適用するにあたり，アクションの発火イベントを状態として保持するMarking判定LTS $E_M$ を用いて，アクションベースの目標を等価な状態ベースの目標へと帰着させる．

合成対象のコンポーネント集合を $\mathcal{E} = \{E_1, \dots, E_n\}$ とし，全コンポーネントのアクション集合の和を $A = \bigcup_{i=1}^n A_{E_i}$ とする．
このとき，$E_M = (S_{E_M}, s_{E_M,0}, S^I_{E_M}, S^M_{E_M}, A_{E_M}, A^M_{E_M}, \Delta_{E_M})$ は以下のように定義される：

\begin{itemize}
  \item $S_{E_M} = \{0, 1\}$, $s_{E_M,0} = 0$, $S^I_{E_M} = \emptyset$, $S^M_{E_M} = \{1\}$
  \item $A_{E_M} = A$ （同期のために全アクションをアルファベットに含める）
  \item 遷移関係 $\Delta_{E_M}$：
        \begin{itemize}
          \item 任意の $a \in A^M$ に対し，$(0, a, 1) \in \Delta_{E_M}$ および $(1, a, 1) \in \Delta_{E_M}$
          \item 任意の $a \notin A^M$ に対し，$(0, a, 0) \in \Delta_{E_M}$ および $(1, a, 0) \in \Delta_{E_M}$
        \end{itemize}
\end{itemize}

さらに，並列合成 $E_\parallel = E_1 \parallel \dots \parallel E_n \parallel E_M$ において目標達成の判定を $E_M$ に一任するため，他のすべてのコンポーネント $E_i \in \{E_1, \dots, E_n\}$ について，そのMarked状態集合を $S^M_{E_i} = S_{E_i}$ （全状態をMarkedとする）へ再定義する．

この変換により，合成システム $E_\parallel$ がMarked状態にあることは，$E_M$ が状態 $1$ にあることと等価になる．$E_M$ は直前に実行されたアクションが $A^M$ に属する場合のみ状態 $1$ に遷移するため，このMarked状態を無限回訪問することは，元のシステムにおいてMarkingアクションを無限回発火し続けるNon-blockingな振る舞いと完全に一致する．

\subsection{On-The-Fly探索アルゴリズム}

従来のアプローチでは，まずオートマトンを完全に合成し，その後で制御問題を解く．
しかし，この方法は指数的な状態空間爆発を引き起こす可能性がある．
Ciolekらは，この課題に対し，ヒューリスティック関数に導かれたOn-The-Fly探索により，状態空間の一部のみを探索してDirected Controllerを発見するアルゴリズムを提案した~\cite{DirectedControllerSynthesis}．
ヒューリスティック関数は，各状態から目標までの推定距離を計算することで，有望な探索経路を優先的に選択する手法であり~\cite{Heuristic}，DCSにおいても探索効率の向上に重要な役割を果たす．

On-The-Fly探索は，状態空間全体を事前に構築することなく，初期状態から到達可能な必要な部分のみを段階的に展開しながら制御器を合成する手法である．
この手法は，主に以下の3つのプロセスから構成される．

\begin{enumerate}
  \item \textbf{展開（Expansion）}:
        現在の探索フロンティアから，ヒューリスティック関数（例えばRA）を用いて最も有望な未探索の遷移を選択し，探索グラフに追加する（\textsc{ExpandNext}）．これにより，Marked状態への到達可能性が高い経路を優先的に探索する．
  \item \textbf{伝播（Propagation）}:
        ある状態が\textit{Goals}（勝利状態）または\textit{Errors}（敗北状態）であることが確定した場合，その情報を探索グラフの逆方向（親状態）へ伝播させる（\textsc{PropagateGoal}, \textsc{PropagateError}）．
        状態の分類は以下の論理に基づき決定される：
        \begin{itemize}
          \item \textbf{Goalsへの伝播}：
                ある状態から制御可能な遷移によって少なくとも1つの\textit{Goals}状態へ到達可能であるか，あるいはその状態から発生しうる全ての制御不能な遷移の先が\textit{Goals}である場合，その状態は\textit{Goals}となる．
          \item \textbf{Errorsへの伝播}：
                ある状態から発生しうる全ての制御可能な遷移の先が\textit{Errors}かつ制御不能な遷移によって少なくとも1つの\textit{Errors}状態へ到達可能である場合，その状態は\textit{Errors}となる．
        \end{itemize}
  \item \textbf{閉路検出と判定（Loop Detection）}:
        探索中に新たな閉路が形成された場合，その閉路がNon-Blockingを満たすか否かを判定する．
        Markingアクションを含む閉路などの「勝利閉路」が見つかれば，その閉路上の状態は\textit{Goals}となり，逆にMarkingアクションを含まず脱出不可能な閉路は\textit{Errors}となる．
\end{enumerate}

探索は初期状態が\textit{Goals}または\textit{Errors}に分類されるまで継続される．初期状態が\textit{Goals}に分類された場合，探索グラフから有効な部分グラフを抽出することで制御器が得られる．逆に\textit{Errors}に分類された場合は，制御器の合成は不可能であると結論付けられる．

\begin{algorithm}
  \caption{On-The-Fly探索によるDirected Controller Synthesis}
  \label{alg:dcs}
  \begin{algorithmic}[1]
    \Require{全てのLTSの組 $\bm{E} = (E_1, \ldots, E_n)$，ヒューリスティック関数 $H$}
    \Ensure{Directed Controller $C$ または 合成不可能}

    \Function{DirectedControllerSynthesis}{$\bm{E}, H$}

    \State $\mathit{ES} \gets$初期状態$\bm{s}_0$のみを持つ部分探索グラフ
    \State $\mathit{Goals} \gets \emptyset, \mathit{Errors} \gets \emptyset, \mathit{None} \gets \{\bm{s}_0\}$

    \Statex

    \LComment{探索ループ：初期状態が未分類状態でなくなるまで}
    \While{$\bm{s}_0 \notin (\mathit{Goals} \cup \mathit{Errors})$}
      \State $(\bm{s}, a, \bm{s}') \gets \textsc{ExpandNext}(\mathit{ES}, H)$ \Comment{ヒューリスティックによる次遷移の選択}
      \State $\mathit{ES} \gets \mathit{ES} \cup \{(\bm{s}, a, \bm{s}')\}$ \Comment{探索グラフの更新}

      \Statex
      \If{$\bm{s}' \in \mathit{Errors}$}
        \State $\mathit{Errors} \gets \mathit{Errors} \cup \{\bm{s}'\}$
        \State $\textsc{PropagateError}(\bm{s}')$ \Comment{敗北状態の伝播}
      \ElsIf{$\bm{s}' \in \mathit{Goals}$}
        \State $\mathit{Goals} \gets \mathit{Goals} \cup \{\bm{s}'\}$
        \State $\textsc{PropagateGoal}(\bm{s}')$ \Comment{勝利状態の伝播}
      \ElsIf{$\bm{s}'$ が新しいループを形成する}
        \State $\mathit{Loop} \gets \textsc{GetLoop}(\bm{s}, \bm{s}')$
        \If{$\mathit{Loop}$ が勝利条件を満たす}
          \State $\mathit{newGoals} \gets \textsc{FindNewGoals}(\mathit{Loop})$
          \State $\mathit{Goals} \gets \mathit{Goals} \cup \mathit{newGoals}$
          \State $\textsc{PropagateGoal}(\mathit{newGoals})$
        \Else
          \State $\mathit{newErrors} \gets \textsc{FindNewErrors}(\mathit{Loop})$
          \State $\mathit{Errors} \gets \mathit{Errors} \cup \mathit{newErrors}$
          \State $\textsc{PropagateError}(\mathit{newErrors})$
        \EndIf
      \EndIf
    \EndWhile

    \Statex

    \If{$s_0 \in \mathit{Goals}$}
      \State $C \gets \textsc{ExtractController}(\mathit{ES}, \mathit{Goals})$
      \Return $C$
    \Else
      \Return 合成不可能
    \EndIf

    \EndFunction

  \end{algorithmic}
\end{algorithm}

\section{Ready Abstraction}

Ready Abstraction（RA）~\cite{ReadyAbstraction}は，並列合成構造を活用して，Marked状態への到達に必要なステップ数を多項式時間で推定するヒューリスティック手法である．
各コンポーネントの局所情報のみを用いて距離を推定することで状態空間爆発を回避しつつ，On-The-Fly探索における次遷移選択（\textsc{ExpandNext}）を導く．

RAの基本的な着想は，合成状態 $\bm{s} = (s_{E_1}, s_{E_2}, \ldots, s_{E_n})$ の周辺で局所的に発火可能なアクション（Readyアクション）を集め，それらの間に，あるアクションの実行が別のアクションの実行可能性を高めるという関係を仮想的に張ったグラフ上で最短路を解くことで，目標（Marked状態）までの距離を見積もる点にある．
この手法は，プランニング問題におけるヒューリスティック探索の一般的な枠組み~\cite{Heuristic}を，並列合成されたDESの構造に適合させたものと位置づけられる．
同期制約のため，局所的に発火可能（Ready）であっても合成状態全体では直ちに発火できない場合があるが，RAはこの差を局所到達可能性とグラフ探索に基づいて保守的に見積もる．

\subsection{展開対象のアクションの評価}

\aref{alg:ra_heuristic}は，現在の合成状態 $\bm{s}$ において展開対象のアクション $\hat{a}$ を選ぶことがどれだけ目標に近いかを評価するためのヒューリスティック関数である．
評価は各コンポーネント $E_i$ ごとに $(\mathrm{rank}, d)$ の形で返され，rank は距離推定に用いる目標集合の優先度を表す．

本章で示すRAの記述では，探索中にすでに到達したMarked状態集合 $\bm{S}^M$ を基準にした距離（Rank 0）を優先する．
これは，探索が進むほど既知の到達済み領域へ近づく選択を上位に扱うことで，任意のMarked状態を一様に目指す場合に比べて探索が過度に広がることを避け，探索空間を抑制できるという見込みに基づく．
Rank 0での推定が不可能な場合に限り，任意のMarked状態への距離（Rank 1）を用いる．
どちらも不可能と推定された場合は $(2,\infty)$ を返す．

\begin{algorithm}
  \caption{Ready Abstractionによるヒューリスティック関数}
  \label{alg:ra_heuristic}
  \begin{algorithmic}[1]
    \Require{%
      全てのLTSの組 $\bm{E} = (E_1, E_2, \ldots, E_n)$， \\
      探索到達済みのMarked状態の集合 $\bm{S}^M \subseteq S^M_{E_1} \times S^M_{E_2} \times \cdots \times S^M_{E_n}$， \\
      各LTSにおける状態の組 $\bm{s} = (s_{E_1}, s_{E_2}, \ldots, s_{E_n})$， \\
      展開対象のアクション $\hat{a}$
    }
    \Ensure{%
      各LTSにおける距離の降順の組 $\bm{d} = (\bm{d}_1, \bm{d}_2, \ldots, \bm{d}_n)$ \\
      ただし，$\bm{d}_i \in \{(0, d), (1, d), (2, \infty) \mid d \in \mathbb{N}\}$ \\
      各LTSにおける距離の意味は以下の通り： \\
      $(0, d)$：次にアクション$\hat{a}$を発火して$d$ステップで探索到達済みのMarked状態に到達可能 \\
      $(1, d)$：次にアクション$\hat{a}$を発火して$d$ステップでMarked状態に到達可能 \\
      $(2, \infty)$：Marked状態に到達不可能
    }

    \Function{ReadyAbstractionHeuristic}{$\bm{E}, \bm{S}^M, \bm{s}, \hat{a}$}

    \For{$i = 1, 2, \ldots, n$}
      \State $\mathit{minSteps} \gets \infty$

      \Statex

      \LComment{Rank 0: 探索到達済みのMarked状態への到達可能性確認}
      \ForAll{$(s^*_{E_1}, s^*_{E_2}, \ldots, s^*_{E_n}) \in \bm{S}^M$}
        \State $\mathit{steps} \gets \textsc{EstimateDistToState}(\bm{s}, \hat{a}, s^*_{E_i})$
        \If{$\mathit{steps} < \mathit{minSteps}$}
          $\mathit{minSteps} \gets \mathit{steps}$
        \EndIf
      \EndFor
      \If{$\mathit{minSteps} \ne \infty$} \Comment{いずれかの探索到達済みのMarked状態に到達可能な場合}
        \State $\bm{d}_{E_i} \gets (0, minSteps)$
        \Continue
      \EndIf

      \Statex

      \LComment{Rank 1: 任意のMarked状態への到達可能性確認}
      \ForAll{$s^*_{E_i} \in S^M_{E_i}$}
        \State $\mathit{steps} \gets \textsc{EstimateDistToState}(\bm{s}, \hat{a}, s^*_{E_i})$
        \If{$\mathit{steps} < \mathit{minSteps}$}
          $\mathit{minSteps} \gets \mathit{steps}$
        \EndIf
      \EndFor

      \If{$\mathit{minSteps} \ne \infty$} \Comment{いずれかのMarked状態に到達可能な場合}
        \State $\bm{d}_{E_i} \gets (1, minSteps)$
      \Else
        \State $\bm{d}_{E_i} \gets (2, \infty)$
      \EndIf
    \EndFor

    \Statex

    \Return $\textsc{SortDescending}((\bm{d}_{E_1}, \bm{d}_{E_2}, \ldots, \bm{d}_{E_n}))$
    \Comment{距離を辞書式降順でソートして返す}

    \EndFunction
  \end{algorithmic}
\end{algorithm}

\aref{alg:ra_heuristic}の戻り値は各コンポーネントの距離推定の組であるが，実際の探索（\textsc{ExpandNext}）においてアクション $\hat{a}$ を選択する際は，以下の優先順位に従って順位付けを行う：

\begin{enumerate}
  \item \textbf{制御不能アクションの優先}：$\hat{a} \in A^U$ であるアクションを，全ての制御可能アクション $\hat{a} \in A^C$ よりも優先する．
  \item \textbf{辞書式順序による比較}：アクションの属性が同じ（共に制御可能，あるいは共に制御不能）である場合，\aref{alg:ra_heuristic}が返す距離の組 $\bm{d}$ を辞書式に比較し，値が小さい（より目標に近いと推定される）ものを優先する．
\end{enumerate}

制御不能アクションを最優先するのは，環境側で発生しうる全ての振る舞いを早期に探索し，反例（違反状態やデッドロック）を迅速に検出するためである．

\subsection{距離推定の中核}

Ready Abstractionにおける距離推定の手続きを\aref{alg:estimate_dist_to_state}に示す．
この関数 $\textsc{EstimateDistToState}$ は，現在の合成状態 $\bm{s}$，評価対象のアクション $\hat{a}$，および目標状態 $s^*_{E_i}$ を入力とし，推定距離 $d$ を算出する．

本アルゴリズムでは，推定のために以下の2つの補助関数を利用する．

\begin{description}
  \descitem{\textnormal{$\textsc{CalculateDist}(s, a)$}}
  LTS単体において，状態 $s$ からアクション $a$ へ到達するための最短ステップ数．
  この値は探索中頻繁に参照されるため，効率化の観点から，探索開始前にすべての状態とアクションの組について幅優先探索により算出されている．

  \descitem{\textnormal{$\textsc{CalculateGap}(\bm{s}, \hat{a}, a)$}}
  現在の合成状態 $\bm{s}$ において，アクション $\hat{a}$ の実行後に別のアクション $a$ を実行可能にするために必要な最小ステップ数（切り替えコスト）．
  これは動的な状態に依存するため，探索中に都度計算される．
\end{description}

RAは，現在の状態で発火可能なReadyアクション集合 $A^R$ を対象に，そこへ至る遷移コスト（Gap）とその先の局所距離（Dist）の和が最小となる経路を探索する．
これにより，単なる最短距離ではなく，同期のための待機やアクションの切り替えコストを含んだ，より実質的な到達距離を推定している．

\begin{algorithm}
  \caption{Ready Abstractionにおける距離推定関数}
  \label{alg:estimate_dist_to_state}
  \begin{algorithmic}[1]
    \Require{%
      現在の状態 $\bm{s} = (s_{E_1}, \ldots, s_{E_n})$， \\
      評価対象のアクション $\hat{a}$， \\
      距離を推定する対象の状態 $s^*_{E_i}$
    }
    \Ensure{推定距離 $d \in \mathbb{N} \cup \{\infty\}$}

    \Function{EstimateDistToState}{$\bm{s}, \hat{a}, s^*_{E_i}$}

    \If{$\hat{a} \in A_{E_i}$} \Comment{$\hat{a}$ がLTS $E_i$ に存在している場合}
      \State $s'_{E_i} \gets s' \text{ where } (s_{E_i}, \hat{a}, s') \in \Delta_{E_i}$ \Comment{$\hat{a}$ を発火した後の状態}
      \If{$s'_{E_i} = s^*_{E_i}$} \Comment{対象の状態に到達する場合}
        \Return $1$
      \ElsIf{$s'_{E_i} \in S^I_{E_i}$} \Comment{違反状態に到達する場合}
        \Return $\infty$
        \Comment{距離を無限と推定し，Marked状態に到達不可能であることを表す}
      \ElsIf{$s'_{E_i} \ne s_{E_i}$} \Comment{他の状態に遷移する場合}
        \Return $\textsc{CalculateDist}(s'_{E_i}, s^*_{E_i}) + 1$
      \EndIf
    \EndIf

    \Statex
    \LComment{各LTS上で，現在の状態から発火可能なアクション（Readyアクション）を収集}
    \State $A^R \gets \bigcup_{i=1}^{n} \{a \mid \exists s, (s_{E_i}, a, s) \in \Delta_{E_i}, s \neq s_{E_i}, s \notin S^I_{E_i}\}$

    \Statex
    \LComment{候補アクション $\hat{a}$ から構造的に到達可能なアクション集合 $A^*$ を構築}
    \State $A^* \gets \emptyset$
    \For{$i = 1, \ldots, n$}
      \State $s'_{E_i} \gets s' \text{ where } (s_{E_i}, \hat{a}, s') \in \Delta_{E_i}$
      \If{$s'_{E_i} \neq \bot \land s'_{E_i} \notin S^I_{E_i}$}
        \State $A^* \gets A^* \cup \{ a^* \in A^R \mid T_{s'_{E_i} \to \{a^*\}} \neq \emptyset \}$
      \EndIf
    \EndFor

    \Statex
    \LComment{Gapを加味した最短経路探索}
    \State $d \gets \min_{a^* \in A^*}\{\textsc{CalculateGap}(\bm{s}, \hat{a}, a^*) + \textsc{EstimateDistToState}(\bm{s}, a^*, s^*_{E_i})\}$
    \LComment{この再帰的な最小値問題は，実装上はダイクストラ法により効率的に解かれる}

    \Return $d$

    \EndFunction
  \end{algorithmic}
\end{algorithm}

ここで用いられる \textsc{CalculateGap}$(\hat{a}, a)$ は，アクション間の切り替えのしやすさを表すコストであり，既存研究における定義に従い，あるコンポーネント内でアクション $\hat{a}$ を経て $a$ へ至る最短パスの長さ（から1を引いた値）として計算される．

\subsection{Markingアクションまでの距離推定}

RAは，Marked状態への到達距離を推定する枠組みとして提案されている．
一方，本研究では目標をMarkingアクションの発火として定式化するため，RAを本研究の目的関数と同じ観点で比較するには，Marked状態ではなくMarkingアクションまでの距離をどのように見積もるかを明示しておく必要がある．
そこで本節では，Markingアクション集合 $A^M$ に対する距離推定手続きとして\textsc{EstimateDistToMarking}を示す．

\textsc{EstimateDistToMarking}は，候補アクション$\hat{a}$がMarkingアクションであれば距離1を返し，そうでなければ，合成状態$\bm{s}$から（同期を無視すれば）状態を変化させ得るアクション集合$A^R$を介して，Gapとその先での推定距離の和の最小値を再帰的に求める．
構造は\aref{alg:estimate_dist_to_state}（\textsc{EstimateDistToState}）と同型であり，終端条件がMarked状態ではなくMarkingアクションになっている点が相違である．

なお，\textsc{EstimateDistToMarking}は，探索到達済みMarked状態集合$\bm{S}^M$を基準とするRank 0の推定が利用できない状況，特に$\bm{S}^M=\emptyset$の場合に用いることを想定している．


\begin{algorithm}
  \caption{Ready AbstractionにおけるMarkingアクションまでの距離推定}
  \begin{algorithmic}[1]
    \Require{%
      現在の状態 $\bm{s} = (s_{E_1}, \ldots, s_{E_n})$， \\
      候補のアクション $\hat{a}$
    }
    \Ensure{推定距離 $d \in \mathbb{N} \cup \{\infty\}$}

    \Function{EstimateDistToMarking}{$\bm{s}, \hat{a}$}

    \If{$\hat{a} \in A^M$}
      \Return $1$
    \EndIf

    \LComment{各LTS上で，現在の状態から発火可能なアクションのうち，自己ループでないものの集合}
    \For{$j = 1, 2, \ldots, n$}
      \State $A^R_{E_j} \gets \{a \mid \exists s', (s_{E_j}, a, s') \in \Delta_{E_j}, s' \neq s_{E_j}\}$
    \EndFor
    \State $A^R \gets \bigcup_{j=1}^{n} A^R_{E_j}$ \Comment{Readyアクションの集合}

    \Return $\min_{a \in A^R \cup A^M}\{\textsc{CalculateGap}(\hat{a}, a) + \textsc{EstimateDistToMarking}(\bm{s}, a)\}$

    \EndFunction
  \end{algorithmic}
\end{algorithm}

以上により，RAは局所情報から，次にどのアクションを展開するのが有望かを推定する枠組みを提供する．
On-The-Fly探索側では，これらの推定値を用いて展開順序を制御することで，合成状態空間の全探索を避けつつ，目的を満たす制御器の発見を狙う．

\chapter{Ready Abstractionの課題}
\label{chap:ready_abstraction_limitations}

前章で述べたように，Ready Abstraction（RA）は局所的に発火可能なアクション（Readyアクション）間の関係性を利用して，目標までの距離を推定するヒューリスティック手法である．
RAは，コンポーネント間の結合が疎であり，各コンポーネントが比較的自律的に目標へ遷移できるシステムにおいては強力なガイドとなる．

しかし，本研究が対象とするような，Markingアクションの発火に複数のコンポーネントによる厳密な同期が必要となるシステムにおいては，RAの推定精度と計算効率の両面で深刻な課題が生じる．
本章では，RAが抱える構造的な限界について，同期構造の無視による探索効率の低下と，Readyアクション探索に伴う計算コストの増大という2つの観点から分析する．

\section{同期構造の看過に起因する局所的な探索}

RAの最大の特徴は，現在の合成状態において直ちに発火可能なアクション（Readyアクション）のみをノードとするグラフ上で距離計算を行う点にある．
この特性は，将来的に発生する同期イベントを適切に評価できず，大域的な最適性を損なうという欠点につながる．

\subsection{同期待ち時間の無視}

Markingアクション $a_m$ が同期アクションである場合，その発火には関与する全てのコンポーネントが $a_m$ を発火可能な状態に到達していなければならない．
しかし，あるコンポーネント $E_i$ が既に $a_m$ の直前状態に到達していても，他のコンポーネント $E_j$ がまだ準備できていなければ，システム全体として $a_m$ は実行できない．

RAの距離推定アルゴリズム（\aref{alg:estimate_dist_to_state}）は，現在のReadyアクション集合 $A^R$ を起点として，目標までの最短パスを探索する．
このとき，同期が必要なアクションへのパスは，パートナーの到着を待つための遷移（同期待ち）を含める必要があるため，RAのグラフ上では遠い，あるいは到達困難と判定されやすい．
対照的に，同期を必要とせず単独で進行でき，かつ局所的に目標に近い場所へ遷移できるアクションが存在する場合，RAはその局所的な遷移コストの低さを優先して評価する．

RAはこの他者の同期待ちという大域的な制約を考慮できず，各コンポーネントが局所的な最短経路を選択し続けるような局所最適化に基づいた評価を下すため，結果として合流不可能な経路や，効率の悪い状態を優先的に探索する結果となる．

\subsection{具体的な不適合例：金属加工システム}
\label{subsec:ra_metal_processing_example}

RAが不適切な誘導を行う具体的なシナリオとして，\aref{fig:metal_processing_model}に示す金属加工システムのモデルを用いて説明する．

本システムは，加工プロセスを表す環境モデル（$E_\mathit{env}$）と，工程管理を行う要求モデル（$E_\mathit{req}$）の2つのコンポーネントから構成される．
目標は「出荷」アクションの発火（Markingアクション）である．
なお，説明を簡単にするため，本モデルに含まれる全てのアクションは制御可能（Controllable）であるとする．

環境モデルは，初期状態から「金属準備」を経て待機状態（状態1）へ遷移する．この状態1を起点として，2つの経路が存在する．
1つ目は「溶解」，「成形」，「冷却」を行い，再び待機状態に戻る加工サイクルである．
このサイクルは0回以上実行可能である．
2つ目は「研磨」や「皮膜」を施し，最後に「出荷」して初期状態に戻る仕上げ・出荷工程である．

要求モデルは，製品の品質担保のために「成形」が1度以上発火されることを強制する．
具体的には，初期状態（成形未実施，状態0）において「成形」を経ずに「出荷」アクションが発火された場合，未加工品の出荷とみなされ，違反状態$-1$（図下段の赤色ノード）へ遷移する．
「成形」が発火されると状態1（成形済み）へ遷移し，以降は「出荷」を行っても初期状態に戻るだけであり，違反にはならない．

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[shorten >=1pt, node distance=2.5cm, on grid, auto, >={Stealth}]

    % --- 環境モデル (Environment) ---
    \begin{scope}[yshift=0cm]
      \node[anchor=west] at (-3, 1) {\textbf{環境モデル} $E_\mathit{env}$};

      \node[state, initial, initial text=] (e0) {$0$};
      \node[state] (e1) [right=of e0] {$1$};
      \node[state] (e2) [right=of e1] {$2$};
      \node[state] (e3) [right=of e2] {$3$};
      \node[state] (e4) [below=of e1] {$4$};

      \path[->]
      (e0) edge node {\footnotesize 金属準備} (e1)
      (e1) edge node {\footnotesize 溶解} (e2)
      (e2) edge node {\footnotesize 成形} (e3)
      (e3) edge [bend left=45] node {\footnotesize 冷却} (e1)
      (e1) edge node [swap] {\footnotesize 研磨，皮膜} (e4)
      (e4) edge [bend left=45, orange] node {\footnotesize 出荷} (e0);
    \end{scope}

    % --- 要求モデル (Requirement) ---
    \begin{scope}[yshift=-4.5cm]
      \node[anchor=west] at (-3, 1) {\textbf{要求モデル} $E_\mathit{req}$};

      \node[state, draw=red, fill=red!15] (r-1) {$-1$};
      \node[state, initial, initial text=] (r0) [right=of r-1] {$0$};
      \node[state] (r1) [right=of r0] {$1$};

      \path[->]
      (r0) edge node {\footnotesize 成形} (r1)
      (r1) edge [loop right] node {\footnotesize 成形} ()
      (r0) edge [bend left=45, orange] node {\footnotesize 出荷} (r-1)
      (r1) edge [bend left=45, orange] node {\footnotesize 出荷} (r0);
    \end{scope}

  \end{tikzpicture}
  \caption{金属加工システムの環境モデルと要求モデル}
  \label{fig:metal_processing_model}
\end{figure}

\subsubsection{Ready Abstraction適用のためのMarked状態への変換}

RAは本来，Marked状態への到達を目的とする探索手法である．
一方，本問題の目標は「出荷」アクションの発火であるため，\aref{subsec:marking_to_marked}で述べた変換手法を適用し，アクションベースの目標を状態ベースの目標に置き換える必要がある．

\aref{fig:metal_processing_model_marked}は，変換後のシステム構成を示している．
ここでは，新たなコンポーネントとしてMarking判定モデル $E_M$ を導入している．
$E_M$は，「出荷」アクションが発火された直後のみMarked状態（状態1）に遷移し，それ以外の場合は非Marked状態（状態0）に留まる機能を持つ．
いわば，アクションの発火イベントを状態遷移として表現するためのモデルである．

この変換に伴い，既存の環境モデル $E_\mathit{env}$ および要求モデル $E_\mathit{req}$ においては，違反状態を除くすべての状態をMarked状態（図中の二重丸）として再定義する．
これにより，システム全体の並列合成状態がMarkedとなる（全てのコンポーネントが同時にMarked状態にある）ための条件は，実質的に$E_M$が状態1になること，すなわち直前に出荷アクションが発火したことと等価になる．

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[shorten >=1pt, node distance=2.5cm, on grid, auto, >={Stealth}]

    % --- 環境モデル (Environment) ---
    \begin{scope}[yshift=0cm]
      \node[anchor=west] at (-3, 1) {\textbf{環境モデル} $E_\mathit{env}$};

      \node[state, initial, initial text=, accepting] (e0) {$0$};
      \node[state, accepting] (e1) [right=of e0] {$1$};
      \node[state, accepting] (e2) [right=of e1] {$2$};
      \node[state, accepting] (e3) [right=of e2] {$3$};
      \node[state, accepting] (e4) [below=of e1] {$4$};

      \path[->]
      (e0) edge node {\footnotesize 金属準備} (e1)
      (e1) edge node {\footnotesize 溶解} (e2)
      (e2) edge node {\footnotesize 成形} (e3)
      (e3) edge [bend left=45] node {\footnotesize 冷却} (e1)
      (e1) edge node [swap] {\footnotesize 研磨，皮膜} (e4)
      (e4) edge [bend left=45] node {\footnotesize 出荷} (e0);
    \end{scope}

    % --- 要求モデル (Requirement) ---
    \begin{scope}[yshift=-4.5cm]
      \node[anchor=west] at (-3, 1) {\textbf{要求モデル} $E_\mathit{req}$};

      \node[state, draw=red, fill=red!15] (r-1) {$-1$};
      \node[state, initial, initial text=, accepting] (r0) [right=of r-1] {$0$};
      \node[state, accepting] (r1) [right=of r0] {$1$};

      \path[->]
      (r0) edge node {\footnotesize 成形} (r1)
      (r1) edge [loop right] node {\footnotesize 成形} ()
      (r0) edge [bend left=45] node {\footnotesize 出荷} (r-1)
      (r1) edge [bend left=45] node {\footnotesize 出荷} (r0);
    \end{scope}

    \begin{scope}[yshift=-7.5cm]
      \node[anchor=west] at (-3, 1.2) {\textbf{Marking判定モデル} $E_M$};

      \node[state, initial, initial text=] (m0) {$0$};
      \node[state, accepting] (m1) [right=3.5cm of m0] {$1$};

      \path[->]
      (m0) edge [loop below] node {\footnotesize $A_{E_\mathit{env}} \setminus \{\text{出荷}\}$} ()
      (m1) edge [bend left=20] node {\footnotesize $A_{E_\mathit{env}} \setminus \{\text{出荷}\}$} (m0)
      (m0) edge [bend left=20] node {\footnotesize 出荷} (m1)
      (m1) edge [loop below] node {\footnotesize 出荷} ();
    \end{scope}

  \end{tikzpicture}
  \caption{アクション目標を状態目標へ変換した金属加工システム}
  \label{fig:metal_processing_model_marked}
\end{figure}

\subsubsection{RAによる探索と失敗のプロセス}

前目で述べた変換後の金属加工システム（\aref{fig:metal_processing_model_marked}）に対し，RAを用いたOn-The-Fly探索を適用した場合の具体的な挙動を示す．
本モデルでは，$E_\mathit{env}$ と $E_\mathit{req}$ の全状態がMarkedと設定されているため，探索の主目的は Mark判定モデル $E_M$ を状態1（Marked）に遷移させること，すなわち「出荷」アクションを発火させることになる．

RAを用いた場合でも，最終的には適切な制御器が合成されるが，その過程で生じる非効率な探索挙動について以下に段階的に示す．

\begin{description}
  \descitem{Step 1: 初期状態からの遷移}
  初期状態 $\bm{s}_0 = (0, 0, 0)$ （順に $E_\mathit{env}, E_\mathit{req}, E_M$ の状態）において，発火可能なアクションは金属準備のみである．
  システムはこれを選択し，状態 $\bm{s}_1 = (1, 0, 0)$ へ遷移する．
  このとき，$E_\mathit{env}$ は状態1（分岐点）に進むが，$E_\mathit{req}$ と $E_M$ は金属準備に関与しないため状態0に留まる．

  \descitem{Step 2: RAによるヒューリスティック評価と誤った選択}
  状態 $\bm{s}_1$ において，RAは次に選択すべきアクションを決定する．
  この時点で局所的に発火可能なアクション（Readyアクション）の集合は，$A^R = \{\text{研磨}, \text{皮膜}, \text{溶解}, \text{成形}, \text{出荷}\}$ である．

  まず，環境モデル $E_\mathit{env}$ および要求モデル $E_\mathit{req}$ の評価を行う．
  これらは設定により全状態がMarkedであるため，現在の状態が既に目標であるとみなされる．したがって，これらのコンポーネントにおける距離推定値は $1$ となり，アクション選択の差別化要因とならない．

  したがって，評価の差が生じるのは Mark判定モデル $E_M$ における推定である．
  $E_M$ がMarked状態（状態1）へ遷移するためには，同期アクションである「出荷」の発火が不可欠である．
  RAは，展開対象のアクション $\hat{a}$ から「出荷」へのGap（遷移コスト）と，その先の $E_M$ における距離（Dist）の和として値を算出する．

  \begin{description}
    \descitem{選択肢A：「研磨」または「皮膜」}
    環境モデル $E_\mathit{env}$ において，「研磨」または「皮膜」から「出荷」へのGapは $1$ である．
    一方，$E_M$ において「出荷」発火後の状態はMarked状態そのものであるため，その距離（Dist）は $1$ である．
    よって，推定距離は $d = 1 + 1 = 2$ と算出される．

    \descitem{選択肢B：「溶解」}
    環境モデル $E_\mathit{env}$ において，「溶解」から「成形」「冷却」を経て「出荷」に至る最小のGapは $3$ である．
    $E_M$ における距離（Dist）は同様に $1$ である．
    よって，推定距離は $d = 3 + 1 = 4$ と算出される．
  \end{description}

  RAはこの推定値に基づき，距離の小さい（$2 < 4$）「研磨」および「皮膜」を優先すべき有望なアクションと判断する．
  その結果，アルゴリズムは正解ルートである「溶解」を後回しにし，まずは局所的な算出コストが低い「研磨」を選択して状態 $\bm{s}_2 = (4, 0, 0)$ への遷移を行う．

  \descitem{Step 3: 経路の行き詰まりと繰り返される不要な探索}
  遷移後の状態 $\bm{s}_2 = (4, 0, 0)$ において，環境モデル上では出荷が可能である．
  しかし，要求モデル $E_\mathit{req}$ は依然として状態0（成形未実施）にある．
  この状態で出荷を発火すると，$E_\mathit{req}$ は違反状態（$-1$）へ遷移してしまうため，DCSの安全性制約によりこの遷移は禁止される．
  他に有効なアクションも存在しないため，この経路は探索の行き詰まりとなる．

  この結果，探索アルゴリズムはこの経路を破棄し，分岐点である状態 $\bm{s}_1$ までバックトラックを行う．
  しかし，RAのヒューリスティック評価では，残る選択肢である「皮膜」も「溶解」よりコストが低いと判定されている．
  そのため，アルゴリズムは「溶解」を選ぶ前に，もう一方の選択肢である「皮膜」の探索へ移行する．
  「皮膜」ルートも同様に同期待機ができず行き詰まりとなるため，再びバックトラックが発生する．

  このように，RAのヒューリスティック値に従い2つの冗長な経路を探索し尽くした後に，ようやく次善の選択肢であった「溶解」ルートの探索が開始される．
\end{description}

以上のプロセスにより，RAを用いた探索では，同期（$E_\mathit{req}$ の状態進行）の必要性を見抜けずに，局所的に目標アクション「出荷」へ近づきやすい経路を優先して探索する．
大規模なシステムにおいては，このような冗長な探索とバックトラックが頻発し，計算リソースを著しく浪費する原因となる．

\section{網羅的な距離推定による計算資源の浪費}

RAのもう一つの課題は，ヒューリスティック関数の評価プロセスそのものに内在する計算の冗長性と，それに伴うコストの増大である．

前章の\aref{alg:ra_heuristic}で示した通り，RAは1回の探索ステップ（\textsc{ExpandNext}）において，システムを構成する全てのコンポーネント $E_i \; (i=1, \dots, n)$ に対し，以下の2段階の評価を網羅的に実行する．

\begin{enumerate}
  \item Rank 0の評価: 探索中に到達済みのMarked状態への距離を計算する．
  \item Rank 1の評価: 上記が到達不能な場合，モデル上の全てのMarked状態への距離を計算する．
\end{enumerate}

すなわち，アクションを1つ選択するたびに，最大で $2n$ 回の距離推定関数（\textsc{EstimateDistToState}）が呼び出されることになる．
ここで呼び出される距離推定関数は，\aref{chap:background}で述べた通り，Readyアクショングラフ上の探索を行う処理である．
単一の計算は多項式時間で完了するが，状態や遷移に依存した動的な計算処理を伴う．

問題となるのは，この計算が現在の状態遷移に関与しないコンポーネントを含めた全てに対し，一律に実行される点である．
大規模な並列システムにおいて，ある瞬間に状態遷移のボトルネックとなっているコンポーネントは全体の一部に限られ，他は待機状態や無関係な状態にあることが多い．
しかし，RAはそのような文脈を考慮せず，全てのコンポーネントに対して毎回同様に探索計算を行う．

システム規模が拡大しコンポーネント数 $n$ が増加すると，この網羅的な探索プロセスは，探索の進行に寄与しない冗長な計算を繰り返すことを意味する．
結果として，本来不要なヒューリスティック値の算出に計算機資源を費やすことになり，大規模システムにおける合成全体の効率を低下させる要因となる．

\section{本章のまとめ}

本章では，Markingアクションを目標とする離散制御器合成において，従来のReady Abstractionが抱える課題を分析した．

第一の課題は同期の無視である．RAは局所的な最短経路を優先するため，同期のための準備動作よりも，単独で進行可能な近道を過剰評価する．これにより，パートナーの到着を待てずに脇道へ逸れる探索（不要な探索）が発生する．

第二の課題は計算の冗長性である．RAは距離推定のために，現在の状態に関与しないコンポーネントも含めて最大 $2n$ 回の計算を一律に行うため，システム規模に対して不必要な計算資源を浪費する．

これらの課題は，RAが同期構造に対する大域的な視点を欠いていることと，全コンポーネントを等しく評価対象とする網羅的な計算構造に起因する．
次章では，これらの問題を解決するために，目標達成に不可欠なアクションをPre-Markingアクション（PMA）として定義し，これを中間目標（ランドマーク）として導入する新たな手法「Landmark Ready Abstraction」を提案する．
本手法は，PMAへの到達を優先することで同期を適切に指向すると同時に，評価対象を解決すべきPMAのみに厳選することで計算の効率化を図る．

\chapter{Landmark Ready Abstraction}
\label{chap:landmark_ready_abstraction}

前章で指摘した通り，Ready Abstraction (RA)は，並列合成されたシステムにおける局所的な遷移可能性を利用して距離を推定する強力な手法である．
しかし，RAは現在の状態から局所的に遷移可能な方向を優先して評価するため，目標達成において将来的に不可避となる同期イベントを見落とし，非効率な探索経路を選択しやすいという構造的な課題を有する．

本章では，この課題を解決するため，新たなヒューリスティック手法であるLandmark Ready Abstraction (LRA)を提案する．
LRAは，RAが持つ動的なグラフ探索の枠組みを継承しつつ，システムの大域的な構造解析に基づいて抽出された中間目標としてのPre-Markingアクションを導入する．
これにより，LRAはRAの利点である遷移コストの精密な見積もりと，静的解析による大域的な方向付けを統合し，複雑な同期構造を持つシステムに対しても効率的な探索を実現する．

\section{Pre-Markingアクションの定義と階層構造}

DCSにおいて，探索の最終目標はMarkingアクションの発火である．
しかし，大規模な並列システムにおいて，初期状態から最終目標までの距離は膨大であり，単一の目標のみを指針とした探索は，広大な状態空間の中で方向を見失うリスクが高い．
そこで本研究では，最終目標に至る過程で構造上回避することのできないアクションをPre-Markingアクションとして定義し，これを最終目標へ至るための段階的な中間目標として利用する．

\subsection{必須アクション}

Pre-Markingアクションを定義するための基礎概念として，ある目標に対する不可避性を表す必須アクションを定義する．

必須アクションとは，ある状態から目標となるアクションに至るいかなるトレースを選択したとしても，その過程で必ず実行しなければならないアクションのことである．
これは，システムが目標に到達するために回避することのできない，構造上のボトルネックや前提条件と解釈できる．

\aref{chap:background}で定義した有効トレース集合 $T_{s \to A^\mathit{Target}}$ を用いて，必須アクション集合 $M^R_E$ を以下のように定義する．

\begin{definition}[必須アクション]
  状態 $s$ における $A^\mathit{Target}$ に対する必須アクション集合 $M^R_E(s, A^\mathit{Target})$ は，目標へ至る全ての有効なトレースにおいて共通して出現するアクション（目標自身を除く）として定義される．

  $$ M^R_E(s, A^\mathit{Target}) = \left( \bigcap_{t \in T_{s \to A^\mathit{Target}}} A_t \right) \setminus A^\mathit{Target} $$

  ここで，$A_t$ はトレース $t$ に含まれるアクションの集合を表す．

\end{definition}

この定義により，$M^R_E(s, A^\mathit{Target})$ に含まれるアクションが未発火である限り，$s$ から $A^\mathit{Target}$ への到達は不可能であることが保証される．

\subsection{Pre-Markingアクションの階層定義}

必須アクションの概念を用い，システムの最終目標から逆算された依存関係の階層を構築する．
本手法では，階層を以下の2段階に区分して定義する．

\begin{definition}[Primary Pre-Markingアクション]
  システム全体の最終目標であるMarkingアクション集合 $A^M$ に対する必須アクションをPrimary Pre-Markingアクションと呼ぶ．
  各状態 $s$ における Primary Pre-Markingアクション集合 $M^\mathit{PP}_E(s)$ は以下のように定義される．
  $$ M^\mathit{PP}_E(s) = M^R_E(s, A^M) \cap M^R_E(s_{E,0}, A^M) $$
\end{definition}

この定義において，$M^R_E(s_{E,0}, A^M)$ は初期状態において大域的に特定される必須アクションであり，$M^R_E(s, A^M)$ は現在の状態 $s$ から見て必須となるアクションである．
これらの積集合をとることで，$M^\mathit{PP}_E(s)$ は，構造上あらかじめ決定されたボトルネックのうち，現時点で未解決のもののみを保持する集合となる．

\begin{definition}[Secondary Pre-Markingアクション]
  ある特定のPre-Markingアクション $a^*$ に対する必須アクションをSecondary Pre-Markingアクションと呼ぶ．
  各状態 $s$ において，特定の $a^*$ に対して必須となるアクション集合 $M^\mathit{SP}_E(s, a^*)$ は以下のように定義される．
  $$ M^\mathit{SP}_E(s, a^*) = M^R_E(s, \{a^*\}) \cap M^R_E(s_{E,0}, \{a^*\}) $$
\end{definition}

ここで，Primary Pre-MarkingアクションがいずれかのMarkingアクションへの到達という選言的な目標に対する必須要件であるのに対し，Secondary Pre-Markingアクションは特定のPre-Markingアクション $a^*$ への到達という単一の目標に対する必須要件である．
本研究では，これらを総称してPre-Markingアクション（PMA）と呼び，探索の指針として用いる．

\section{Pre-Markingアクションの抽出アルゴリズム}

前節の定義に基づき，各状態におけるPMAを抽出するアルゴリズムについて述べる．
なお，本アルゴリズムは探索開始前に一度だけ実行される静的解析である．
定義に従って全ての状態とアクションの組み合わせを網羅的に検査することは，計算コストの観点から現実的ではない．
そのため，本手法では以下の2段階からなる効率的な抽出手法を採用する．

\subsection{必須アクションの抽出}

まず，任意の目標に対する必須アクションを抽出する汎用的な手続きについて述べる．
本手続きは，計算効率を考慮し，最短パス情報による候補集合の限定と，逆方向探索による必須性の判定という2つの工程で構成される．

\begin{description}
  \descitem{Step 1: 最短パスによる候補集合の限定}
  必須アクションは，目標に至るいかなる経路にも含まれるという性質を持つため，初期状態から目標への最短パス上にも必ず存在する．
  したがって，初期状態から目標への最短パスを算出し，そこに含まれるアクションのみを必須アクションの候補とすることで，検査対象となるアクション数を大幅に削減できる．

  \descitem{Step 2: 逆方向探索による必須性の判定}
  限定された候補アクション $a$ が真に不可避であるかを確認するため，アクション $a$ を経由せずに目標へ到達可能か否かを判定する．
  これは，目標のアクションを発火可能な状態集合を起点とし，遷移を逆方向に辿る探索を行うことで効率的に実行できる．
  この逆探索によって初期状態 $s_{E,0}$ に到達可能である場合，アクション $a$ を回避する経路が存在することを意味するため，$a$ は必須ではないと判断される．
  逆に，$s_{E,0}$ に到達できない場合，$a$ は初期状態において目標アクションの発火のための必須アクションであると確定する．
\end{description}

このアルゴリズムを\aref{alg:extract_required}に示す．
なお，本アルゴリズムはPMAの抽出を目的としているため，初期状態において必須でないアクションは，各状態における必須性に関わらず除外する．
したがって，得られる結果 $M$ は，必須アクションの定義における $M^R_E(s, A^\mathit{Target}) \cap M^R_E(s_{E,0}, A^\mathit{Target})$ の条件を満たす集合となる．

\begin{algorithm}
  \caption{必須アクションの抽出}
  \label{alg:extract_required}
  \begin{algorithmic}[1]
    \Require{LTS $E = (S_E, s_{E,0}, S_E^I, A_E, \Delta_E)$，目標アクション集合 $A^\mathit{Target} \subseteq A_E$}
    \Ensure{%
      各状態における必須アクション集合 $M: S_E \to 2^{A_E}$ \\
      ただし $M(s) = M^R_E(s, A^\mathit{Target}) \cap M^R_E(s_{E,0}, A^\mathit{Target})$
    }

    \Function{ExtractRequiredActions}{$E, A^\mathit{Target}$}
    \ForAll{$s \in S_E$}
      $M(s) \gets \emptyset$
    \EndFor

    \Statex
    \LComment{Step 1: 最短パス情報による候補集合の限定}
    \State $\hat{A}_{E} \gets A_{E}$
    \ForAll{$a^* \in A^\mathit{Target}$}
      \State $t \gets \operatorname*{argmin}_{t \in T_{s_{E,0} \to \{a^*\}}} |t|$
      \State $\hat{A}_{E} \gets \hat{A}_{E} \cap A_t$
      \Comment{全ての目標への最短パスに含まれるアクションのみを候補とする}
    \EndFor

    \Statex
    \LComment{Step 2: 逆方向探索による必須性の判定}
    \For{$\hat{a} \in \hat{A}_{E}$}
      \LComment{アクション $\hat{a}$ を通らずに目標に到達可能な状態（不要状態）を探索}
      \State $S^* \gets \{ s \mid \exists a^* \in A^\mathit{Target}, \exists s', (s, a^*, s') \in \Delta_E \land s' \notin S^I_E \}$
      \State $S^*_{\mathit{new}} \gets S^*$

      \While{$S^*_\mathit{new} \neq \emptyset$}
        \State $S^*_\mathit{next} \gets S^*$
        \ForAll{$s^* \in S^*_\mathit{new}$}
          \LComment{$s^*$ へ遷移可能な親状態 $s$ を探索（逆伝播）}
          \ForAll{$(s, a, s^*) \in \Delta_E$}
            \If{$a \neq \hat{a}$}
              \State $S^*_\mathit{next} \gets S^*_\mathit{next} \cup \{s\}$
            \EndIf
          \EndFor
        \EndFor
        \State $S^*_\mathit{new} \gets S^*_\mathit{next} \setminus S^*$
        \State $S^* \gets S^*_\mathit{next}$
      \EndWhile

      \LComment{初期状態から不可避であれば，必須状態に $\hat{a}$ を割り当て}
      \If{$s_{E,0} \notin S^*$}
        \For{$s \in S_E \setminus S^*$}
          \State $M(s) \gets M(s) \cup \{\hat{a}\}$
        \EndFor
      \EndIf
    \EndFor

    \Return $M$
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\subsection{階層構造の構築}

次に，\aref{alg:extract_required}で定義した必須アクション抽出関数を用い，システム全体におけるPre-Markingアクションの階層構造を構築する．
本アルゴリズムは，Primary Pre-Markingアクションの抽出と，それに続くSecondary Pre-Markingアクションの連鎖的な抽出の2段階からなる．

第一段階では，Markingアクション集合 $A^M$ を目標としてPrimary PMA $M^\mathit{PP}_E$ を抽出する．
ここで，Markingアクションが特定のコンポーネントのみに関連する場合や同期アクションである場合を考慮し，Markingアクション集合全体を含むLTSのみを抽出対象とする．
これにより，目標との関連性が薄いコンポーネントにおける誤検出を回避する．

第二段階では，抽出されたPrimary PMAを起点として，Secondary PMA $M^\mathit{SP}_E$ の抽出を行う．
ここでは，あるコンポーネントで発見された必須アクションが，同期を介して他のコンポーネントにおける必須アクションとなる依存関係を網羅する必要がある．
そのために，不動点反復に基づくアプローチを採用する．
具体的には，初期状態で有効なPMAを探索すべき目標集合として管理し，未処理の目標に対して各LTSで必須アクション抽出を実行する．
そこで新たに初期状態で有効な必須アクションが発見された場合，それを新たな目標として集合に追加する．
この操作を新たな目標が発見されなくなるまで繰り返すことで，システム全体に跨る多段階の依存関係を漏らさず抽出する．

このシステム全体に対する階層構築アルゴリズムを\aref{alg:extract_pre_marking}に示す．

\begin{algorithm}
  \caption{Pre-Markingアクション階層の構築}
  \label{alg:extract_pre_marking}
  \begin{algorithmic}[1]
    \Require{全てのLTSの組 $\bm{E} = (E_1, E_2, \ldots, E_n)$}
    \Ensure{%
      各LTSの各状態におけるPrimary Pre-Markingアクション集合 $M^\mathit{PP}_E: S_E \to 2^{A_E}$， \\
      各LTSの各状態におけるSecondary Pre-Markingアクション集合 $M^\mathit{SP}_E: (S_E \times A_E) \to 2^{A_E}$
    }

    \Function{ExtractPreMarkingActions}{$\bm{E}$}
    \LComment{Step 1: Primary Pre-Markingアクションの抽出}
    \State $A^M \gets \bigcup_{i=1}^{n} A^M_{E_i}$ \Comment{全LTSのMarkingアクション集合の和集合}
    \For{$i = 1, \ldots, n$}
      \If{$A^M \subseteq A_{E_i}$}
        \LComment{Markingアクションを全て含むLTSにおいてのみ抽出}
        \State $M^\mathit{PP}_{E_i} \gets \textsc{ExtractRequiredActions}(E_i, A^M)$
      \Else
        \ForAll{$s \in S_{E_i}$}
          $M^\mathit{PP}_{E_i}(s) \gets \emptyset$
        \EndFor
      \EndIf
    \EndFor

    \Statex
    \LComment{Step 2: Secondary Pre-Markingアクションの連鎖的抽出}
    \State $A^* \gets \bigcup_{i=1}^{n} M^\mathit{PP}_{E_i}(s_{E_i,0})$ \Comment{初期状態で有効なPrimary PMAが初期目標}
    \State $A^*_\mathit{new} \gets A^*$ \Comment{新たに追加された目標集合}
    \While{$A^*_\mathit{new} \neq \emptyset$} \Comment{新たな目標が追加されなくなるまで反復}
      \ForAll{$a^* \in A^*_\mathit{new}$} \Comment{未処理の目標アクションについて}
        \For{$i = 1, \ldots, n$}
          \If{$a^* \in A_{E_i}$}
            \LComment{目標アクション $a^*$ を含むLTSにおいてのみ抽出}
            \State $M\gets \textsc{ExtractRequiredActions}(E_i, \{a^*\})$
            \ForAll{$s \in S_{E_i}$}
              $M^\mathit{SP}_{E_i}(s, a^*) \gets M(s)$
            \EndFor
          \Else
            \ForAll{$s \in S_{E_i}$}
              $M^\mathit{SP}_{E_i}(s, a^*) \gets \emptyset$
            \EndFor
          \EndIf
        \EndFor
      \EndFor
      \LComment{新たに発見されたPMAを次回の目標とする}
      \State $A^*_\mathit{new} \gets \left( \bigcup_{a^* \in A^*_\mathit{new}} \bigcup_{i=1}^{n} M^\mathit{SP}_{E_i}(s_{E_i,0}, a^*) \right) \setminus A^*$
      \State $A^* \gets A^* \cup A^*_\mathit{new}$
    \EndWhile

    \Return $M^\mathit{PP}_{E_i},\; M^\mathit{SP}_{E_i} \quad (i = 1, \ldots, n)$
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\section{Landmark Ready Abstractionによる評価}

本節では，抽出されたPMAを用い，各状態の有望さを定量的に評価するヒューリスティック関数について述べる．
従来のReady Abstraction (RA) は，最終目標への幾何的な距離のみを評価指標としており，その到達過程で構造上不可避となる中間のボトルネックを考慮していない．
そのため，局所的には目標に近づいているように見えても，実際には必須となる手順を回避してしまい，後の探索で行き詰まるような経路を高く評価してしまう可能性がある．

これに対しLandmark Ready Abstraction (LRA) では，抽出されたPMAを，最終目標に至るために経由しなければならない中間目標（ランドマーク）として扱う．
未解決のPMAを距離計算の対象に含めることで，ボトルネックの解消にかかるコストを評価値に反映し，より精度の高い距離推定を実現する．
LRAは距離推定の実行対象を，その時点で解決すべきPMAおよび最終目標のみに限定するため，探索1ステップあたりの計算コストを抑制できるという利点も持つ．

\subsection{ヒューリスティック関数の構成}

LRAにおけるヒューリスティック関数 $\textsc{LandmarkReadyAbstractionHeuristic}$ の全体構成について述べる．
本関数は，最終目標であるMarkingアクションへの到達距離を基準としつつ，抽出されたPMAへの距離を評価に組み込むことで，構造的なボトルネックを反映した値を算出する．

具体的な計算手順は以下の通りである．
まず，基本となる評価値として，システムの最終目標であるMarkingアクション集合 $A^M$ への到達距離を算出する．
これにより，障害物がない理想的な状況下での最短ステップ数が得られる．

次に，現在の状態において解決すべきPMA集合 $A^P$ を構築する．
この集合は，現在の状態で必須となっているPrimary PMAを起点とし，そこから依存関係にあるSecondary PMAを再帰的に探索することで生成される．
具体的には，Primary PMA集合に対し，各アクションの前提となるSecondary PMAを追加する操作を，新たなアクションが追加されなくなるまで繰り返す．
これにより，Primary層だけでなく，より深い階層にある潜在的なボトルネックも網羅的に収集される．

最後に，収集された各PMA $a \in A^P$ について到達距離を算出し，これらを現在の推定距離と比較して最大値を更新する．
ここで，PMAへの距離に $1$ を加算しているのは，PMAが最終目標に至るための通過点であり，その達成後も少なくとも1ステップ以上の遷移が必要であることを評価値に反映させるためである．
並列システムの完了時間は，最も解決に時間を要するボトルネック工程によって律速されるため，これらの距離の最大値を採用することで，システム全体としての実質的な残り距離を見積もる．

このアルゴリズムを\aref{alg:lra_heuristic}に示す．
なお，関数 $\textsc{EstimateDistToActions}$ は次項で定義する距離推定関数であり，指定された状態から目標アクション集合へのGapを考慮した距離を返すものとする．

\begin{algorithm}
  \caption{Landmark Ready Abstractionによるヒューリスティック関数}
  \label{alg:lra_heuristic}
  \begin{algorithmic}[1]
    \Require{%
      全てのLTSの組 $\bm{E} = (E_1, E_2, \ldots, E_n)$， \\
      Primary Pre-Markingアクション写像 $\bm{M}^\mathit{PP} = (M^\mathit{PP}_{E_1}, \ldots, M^\mathit{PP}_{E_n})$， \\
      Secondary Pre-Markingアクション写像 $\bm{M}^\mathit{SP} = (M^\mathit{SP}_{E_1}, \ldots, M^\mathit{SP}_{E_n})$， \\
      現在の状態 $\bm{s} = (s_{E_1}, s_{E_2}, \ldots, s_{E_n})$， \\
      展開対象のアクション $\hat{a}$
    }
    \Ensure{推定距離 $d \in \mathbb{N} \cup \{\infty\}$}

    \Function{LandmarkReadyAbstractionHeuristic}{$\bm{E}, \bm{M}^\mathit{PP}, \bm{M}^\mathit{SP}, \bm{s}, \hat{a}$}

    \LComment{Step 1: 最終目標（Markingアクション）への距離計算}
    \State $A^M \gets \bigcup_{i=1}^{n} A^M_{E_i}$
    \State $d \gets \textsc{EstimateDistToActions}(\bm{s}, \hat{a}, A^M)$

    \Statex
    \LComment{Step 2: Pre-Markingアクション集合の構築と距離更新}
    \State $A^P \gets \bigcup_{i=1}^{n} M^\mathit{PP}_{E_i}(s_{E_i})$
    \State $A^P_\mathit{new} \gets A^P$

    \While{$A^P_\mathit{new} \neq \emptyset$}
      \State $A^P_\mathit{new} \gets \left( \bigcup_{a \in A^P_\mathit{new}} \bigcup_{i=1}^{n} M^\mathit{SP}_{E_i}(s_{E_i}, a) \right) \setminus A^P$
      \State $A^P \gets A^P \cup A^P_\mathit{new}$
    \EndWhile

    \ForAll{$a \in A^P$}
      \State $d' \gets \textsc{EstimateDistToActions}(\bm{s}, \hat{a}, \{a\})$
      \State $d \gets \max(d, d' + 1)$
    \EndFor

    \Return $d$
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\subsection{アクション集合への距離推定}

前節のヒューリスティック関数内で用いられる $\textsc{EstimateDistToActions}$ は，現在の合成状態 $\bm{s}$ において，ある候補アクション $\hat{a}$ を実行した後，指定された目標アクション集合 $A^\mathit{Target}$ のいずれかを発火するまでに必要な最小ステップ数を推定する関数である．

RAにおける距離推定（\aref{alg:estimate_dist_to_state}）が，目標となる状態集合への到達距離を算出するのに対し，本関数は目標となるアクション集合への到達距離を算出する点が異なる．
計算の基本構造はRAと共通しており，現在の状態で発火可能なアクション（Readyアクション）を介して目標へ至る経路を探索し，その過程で生じる遷移コスト（Gap）と再帰的な距離の和を最小化することで値を求める．
具体的な手続きを\aref{alg:estimate_dist_to_actions}に示す．

アルゴリズムの手順は以下の通りである．
まず，候補アクション $\hat{a}$ 自体が目標集合 $A^\mathit{Target}$ に含まれる場合，距離は $1$ と判定される．
そうでない場合，現在の状態で発火可能なReadyアクション集合 $A^R$ を起点として探索を行う．
具体的には，$\hat{a}$ から各Readyアクション $a \in A^R$ への切り替えコスト（Gap）と，その $a$ から目標までの再帰的な距離の和を計算し，その最小値を推定距離とする．

ここで用いられる $\textsc{CalculateGap}(\bm{s}, \hat{a}, a)$ は，RAと同様に，各コンポーネントにおいて $\hat{a}$ から $a$ へ至るための局所的な最大コストに基づいて算出される．
この再帰的な定義により，同期による待機やアクションの切り替えを考慮した，目標までの実質的な距離が見積もられる．

\begin{algorithm}
  \caption{目標アクション集合への距離推定}
  \label{alg:estimate_dist_to_actions}
  \begin{algorithmic}[1]
    \Require{%
      現在の状態 $\bm{s} = (s_{E_1}, \ldots, s_{E_n})$， \\
      候補のアクション $\hat{a}$， \\
      目標アクション集合 $A^\mathit{Target} \subseteq \bigcup_{i=1}^{n} A_{E_i}$
    }
    \Ensure{推定距離 $d \in \mathbb{N} \cup \{\infty\}$}

    \Function{EstimateDistToActions}{$\bm{s}, \hat{a}, A^\mathit{Target}$}

    \If{$\hat{a} \in A^\mathit{Target}$}
      \Return $1$
    \EndIf

    \LComment{各LTS上で，現在の状態から発火可能なアクション（Readyアクション）を収集}
    \State $A^R \gets \bigcup_{i=1}^{n} \{a \mid \exists s, (s_{E_i}, a, s) \in \Delta_{E_i}, s \neq s_{E_i}, s \notin S^I_{E_i}\}$

    \Statex
    \LComment{候補アクション $\hat{a}$ から構造的に到達可能なアクション集合 $A^*$ を構築}
    \State $A^* \gets \emptyset$
    \For{$i = 1, \ldots, n$}
      \State $s'_{E_i} \gets s' \text{ where } (s_{E_i}, \hat{a}, s') \in \Delta_{E_i}$
      \If{$s'_{E_i} \neq \bot \land s'_{E_i} \notin S^I_{E_i}$}
        \State $A^* \gets A^* \cup \{ a^* \in A^R \cup A^\mathit{Target} \mid T_{s'_{E_i} \to \{a^*\}} \neq \emptyset \}$
      \EndIf
    \EndFor

    \Statex
    \LComment{依存関係にあるアクションの中から最短経路を探索}
    \State $d \gets \min_{a^* \in A^*} \{ \textsc{CalculateGap}(\bm{s}, \hat{a}, a^*) + \textsc{EstimateDistToActions}(\bm{s}, a^*, A^\mathit{Target}) \}$

    \Return $d$
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\section{探索挙動の改善例：金属加工システム}

本節では，\aref{subsec:ra_metal_processing_example}で述べた金属加工システムの例題に対し，提案手法であるLRAを適用した場合の探索挙動を示す．
RAにおいて発生した冗長な探索を，LRAがいかにして回避し，同期に不可欠なアクションを見据えた適切な探索を行うかについて詳述する．

\subsection{Pre-Markingアクションの抽出結果}

探索に先立ち，LRAは各LTSの構造解析を行い，Pre-Markingアクション（PMA）の抽出と階層化を行う．

まず，最終目標に対する必須アクションであるPrimary PMAの抽出を行う．
環境モデル $E_\mathit{env}$ においては，初期状態から目標アクション「出荷」へ至るすべての経路において，最初のアクションである「金属準備」が必ず実行される．
したがって，「金属準備」は $E_\mathit{env}$ におけるPrimary PMAとして抽出される．
一方，要求モデル $E_\mathit{req}$ については，初期状態から「出荷」へ至る適法なトレースにおいて，アクション「成形」が必ず一度は発火されなければならない．
「成形」を回避して「出荷」を行う経路は違反状態へ遷移するため，「成形」は $E_\mathit{req}$ におけるPrimary PMAとして抽出される．

続いて，PMAに対する必須アクションであるSecondary PMAの抽出を行う．
抽出されたPrimary PMAのうち，未解決の依存関係を持つ「成形」に着目すると，環境モデル $E_\mathit{env}$ において「成形」を実行可能な状態へ到達するためには，その前段階として「溶解」が必ず実行されなければならない．
したがって，「溶解」は「成形」に対するSecondary PMAとして抽出される．

以上の解析により，探索開始時の初期状態において，システム全体の未発火PMA集合は $\{\text{金属準備}, \text{成形}, \text{溶解}\}$ となり，これらがLRAの評価対象集合 $A^P$ に含まれることになる．

\subsection{分岐点におけるヒューリスティック評価の比較}

RAが誤った選択を行った分岐点である状態 $\bm{s}_1 = (1, 0, 0)$ におけるLRAの挙動を追跡する．
この状態に至る遷移で「金属準備」は既に実行済みであるため，ここでの課題は残るPMAである「成形」および「溶解」をいかに効率よく消化するかにある．

この状態において展開可能なアクションは「研磨」，「皮膜」，「溶解」の3つである．
LRAは，これらの候補アクション $\hat{a}$ それぞれについて，実行後の状態からの最終目標（「出荷」）への距離と未消化PMA（「成形」・「溶解」）への距離を，全コンポーネントの同期構造を考慮したGap計算（$\textsc{EstimateDistToActions}$）により算出し，その最大値を評価値 $d$ とする．

すなわち，評価値は以下の式に基づき決定される．
$$
  d = \max \left(
  \begin{aligned}
       & \textsc{EstimateDistToActions}(\bm{s}_1, \hat{a}, \{\text{出荷}\}),    \\
       & \textsc{EstimateDistToActions}(\bm{s}_1, \hat{a}, \{\text{成形}\}) +1, \\
       & \textsc{EstimateDistToActions}(\bm{s}_1, \hat{a}, \{\text{溶解}\}) +1
    \end{aligned}
  \right)
$$

各選択肢における評価の詳細を以下に示す．

\begin{description}
  \descitem{選択肢A：「研磨」または「皮膜」}
  これらのアクションを選択した場合，環境モデル $E_\mathit{env}$ は状態4へ遷移し，要求モデル $E_\mathit{req}$ は状態0に留まる．

  まず，目標（「出荷」）への距離について確認する．
  この遷移後の状態において，「研磨」または「皮膜」と「出荷」の間のGap（遷移ステップ数）は $1$ である．
  これに目標アクション自身のコスト $1$ を加算し，推定距離は $1+1=2$ となる．

  次に，PMA（「成形」・「溶解」）への距離について確認する．
  状態4から「溶解」や「成形」へ到達するためには，構造的な迂回（「出荷」$\to$「金属準備」$\to \dots$）が必要となる．
  LRAの距離推定はこのコストを検出し，それぞれの距離を以下のように算出する．
  「成形」までのGapは $4$ であり，目標コスト $1$ を加えて推定距離は $5$ となる．
  「溶解」までのGapは $3$ であり，目標コスト $1$ を加えて推定距離は $4$ となる．

  結果として，最も遠いPMA（「成形」）への項が支配的となり，LRA評価値は以下のように算出される．
  $$ d = \max(2, 5, 4) = 5 $$

  \descitem{選択肢B：「溶解」}
  このアクションを選択した場合，環境モデル $E_\mathit{env}$ は状態2へ遷移する．

  まず，目標（「出荷」）への距離について確認する．
  状態2から「出荷」へ至る最短パス（「成形」$\to$「冷却」$\to$「研磨」$\to$「出荷」）に基づき，Gapは $3$ となる．
  これに目標コスト $1$ を加算し，推定距離は $4$ となる．

  次に，PMA（「成形」・「溶解」）への距離について確認する．
  「溶解」自体がPMAであるため，推定距離は $1$ と算出される．
  また，「成形」については，直後の状態からのGapは $1$ と計算される．
  これに目標コスト $1$ を加え，推定距離は $1+1=2$ となる．

  結果として，目標への距離の項が支配的となり，LRA評価値は以下のように算出される．
  $$ d = \max(4, 1, 2) = 4 $$
\end{description}

\subsection{探索の結果}

算出された推定距離を比較すると，「研磨」・「皮膜」の評価値 $5$ に対し，「溶解」の評価値は $4$ となり，より小さい値を示す．
RAは目標（「出荷」）への局所的な近さのみを見て「研磨」（距離2）を推奨したが，LRAはシステム全体としてPMA（「成形」）への到達が困難になるコストを評価値に反映している．
これにより，LRAは局所的な近道である研磨ルートを却下し，必須アクションを確実に消化できる「溶解」ルートを正しく選択する．

結果として，探索アルゴリズムはバックトラックを発生させることなく，初手から正解ルートである「溶解」$\to$「成形」$\to$「冷却」$\to$「出荷」の経路を探索する．
このように，LRAは抽出されたPMAの階層構造と，全モデルを考慮した動的距離推定を組み合わせることで，複雑な同期構造を持つシステムにおいても効率的な探索を実現する．

\chapter{評価}
\label{chap:evaluation}

\section{評価実験}

\todo{複数のシナリオにおいて，PMD・RA・BFSの探索状態数と計算時間を比較し，あわせて各問題でのPMA抽出数も記載する．結果として，PMAが抽出された問題ではPMDがRAを大幅に上回り，逆にPMAがない場合はRAが有利あるいは同等となる傾向を示す記述を行う．（注：現在数値を再計測中だが，傾向は変わらないため構成確認用として一旦旧データを用いて記述する）}

\subsection{実験設定}

\subsection{実験結果}

\section{考察}

\section{RAとの動的切り替え戦略}

\chapter{関連研究}
\label{chap:related_works}

\todo{以下の4点について触れ，本研究の立ち位置を明確にする．(1) 離散制御器合成における状態空間爆発への対処手法の変遷，(2) On-The-Flyを用いた他の分野の話（状態爆発系），(3) RAや機械学習を用いたヒューリスティック手法との対比，(4) 他の分野におけるLandmark Heuristic．}

\chapter{結論}
\label{chap:conclusion}

\section{本論文のまとめ}

\todo{本論文で提案したLRAの総括を行う．}

\section{将来研究}

本研究では，LRAの導入により，評価すべきターゲットをシステムにとって真に不可欠なPMAに絞り込むことで，RAが抱える多重な距離計算の冗長性を解消し，探索効率を大幅に改善した．
今後の展望として，以下の三つの方向性が挙げられる．

第一に，On-The-Fly探索アルゴリズム自体のデータ構造とメモリ管理の最適化である．
実験を通じて，LRAによって探索状態数が削減された場合でも，システム規模が極めて大きい場合には，既訪問状態の保存や照合にかかる管理コストが探索速度の律速要因となる傾向が確認された．
現在の実装ではハッシュセットを用いた明示的な状態管理を行っているが，数百万状態を超える規模の探索においては，メモリ使用量とキャッシュ効率の観点で改善の余地がある．
したがって，今後はBDD（二分決定グラフ）を用いたシンボリックな状態管理手法の導入や，探索済み状態の圧縮表現技術を適用することで，より大規模なシステムに対しても高速かつ省メモリな制御器合成を実現する必要がある．

第二に，PMA抽出アルゴリズムの拡張と汎用化である．
現行の手法では，目標達成に必ず発火しなければならないアクションをPMAとして定義している．
この定義は強力な剪定効果を持つ一方で，複数の加工ルートが存在する場合（OR条件）や，確率的な分岐を含むシステムにおいては，必須アクションが存在せずPMAが抽出されないケースがある．
今後は，必須性の条件を緩和し，複数の候補集合を選言的な中間目標として扱う枠組みや，静的解析と動的探索を組み合わせたより柔軟なボトルネック抽出手法を開発することで，LRAの適用範囲をさらに広げることが期待される．

第三に，探索履歴に基づいたヒューリスティック評価の高度化である．
RAは，探索中に到達済みのMarked状態への距離（Rank 0）を，未到達のMarked状態への距離（Rank 1）よりも優先して評価する仕組みを持つ．
この戦略は，探索を既知の安全なループへ誘導し，Non-Blocking性を早期に確定させる上で有効に機能する場合がある．
現在のLRAは，Markingアクションが複数存在する場合でも，それらを一律の目標集合として扱っている．
そこで，LRAにおいてもRAの知見を取り入れ，既に発火履歴のあるMarkingアクションへの指向性を強化する動的な重み付けを導入することで，探索の安定性と収束速度をさらに向上させることが可能であると考えられる．

\appendix

\backmatter

\chapter{謝辞}

\bibliographystyle{jplain}
\bibliography{references}

\end{document}
